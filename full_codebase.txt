Directory Structure:
Root Directory\:
  full_codebase.txt
  generate_codebase.bat
  generate_codebase.py
backend\:
  .env
  main.py
  requirements.txt
backend\app\:
  __init__.py
backend\app\api\:
  __init__.py
  api.py
backend\app\api\endpoints\:
  __init__.py
  indicators.py
  new_indicators.py
backend\app\core\:
  __init__.py
  config.py
  indicator_config.py
backend\app\db\:
  __init__.py
backend\app\models\:
  __init__.py
  indicators.py
backend\app\services\:
  __init__.py
  composite_indicators_service.py
  dbnom_service.py
  fred_service.py
  historical_ism_data.json
  historical_ism_data.py
  indicator_processing_service.py
  unified_indicator_service.py
  yahoo_finance_service.py
frontend\:
  .env
  .gitignore
  README.md
  package-lock.json
  package.json
  tsconfig.json
frontend\public\:
  favicon.ico
  index.html
  logo192.png
  logo512.png
  manifest.json
  robots.txt
frontend\src\:
  App.css
  App.test.tsx
  App.tsx
  index.css
  index.tsx
  logo.svg
  react-app-env.d.ts
  reportWebVitals.ts
  setupTests.ts
frontend\src\components\:
  CategoryHeader.tsx
  CategoryNavigation.tsx
  CoincidentIndicators.tsx
  IndicatorCard.tsx
  IndicatorCategoryPage.tsx
  IndicatorSection.tsx
  LeadingIndicators.tsx
  index.ts
frontend\src\context\:
  IndicatorContext.tsx
frontend\src\pages\:
  Dashboard.tsx
frontend\src\services\:
  DashboardConfig.ts
  IndicatorService.ts

File Contents:

=== backend\app\__init__.py ===



=== backend\app\api\__init__.py ===



=== backend\app\api\api.py ===

# backend/app/api/api.py

from fastapi import APIRouter
from app.api.endpoints import indicators, new_indicators

router = APIRouter()

# Include the new unified indicators router (preferred)
router.include_router(
    new_indicators.router,
    prefix="/v2/indicators",
    tags=["indicators-v2"]
)

# Keep the old indicators router for backward compatibility
router.include_router(
    indicators.router,
    prefix="/indicators",
    tags=["indicators-legacy"]
)

@router.get("/")
async def root():
    return {
        "message": "Welcome to the Macro Dashboard API",
        "versions": {
            "v1": "/api/indicators (legacy endpoints)",
            "v2": "/api/v2/indicators (new unified endpoints)"
        },
        "endpoints": {
            "health": "/health",
            "docs": "/docs",
            "indicators_v2": "/api/v2/indicators",
            "market_status": "/api/v2/indicators/market-status",
            "categories": "/api/v2/indicators/categories"
        }
    }

=== backend\app\api\endpoints\__init__.py ===



=== backend\app\api\endpoints\indicators.py ===

from fastapi import APIRouter, HTTPException, Query, Path
from app.services.fred_service import FredService
from app.services.dbnom_service import DBNomicsService
from app.services.yahoo_finance_service import YahooFinanceService
from app.services.composite_indicators_service import CompositeIndicatorsService
from app.models.indicators import TimeSeriesData, TimeSeriesPoint, IndicatorSignal
from typing import List, Optional
from datetime import datetime, timedelta
import pandas as pd


router = APIRouter()
fred_service = FredService()
dbnom_service = DBNomicsService()
yahoo_finance_service = YahooFinanceService()
composite_indicators_service = CompositeIndicatorsService()

@router.get("/fred/{series_id}", response_model=TimeSeriesData)
async def get_fred_data(
    series_id: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """
    Get time series data from FRED.
    """
    # Get series info for title and other metadata
    series_info = fred_service.get_series_info(series_id)
    if not series_info:
        raise HTTPException(status_code=404, detail=f"Series {series_id} not found")
    
    # Get series data
    df = fred_service.get_series_data(series_id, start_date, end_date)
    if df.empty:
        raise HTTPException(status_code=404, detail=f"No data found for series {series_id}")
    
    # Convert to response model
    data_points = [
        TimeSeriesPoint(date=row["date"], value=row["value"]) 
        for _, row in df.iterrows() if pd.notna(row["value"])
    ]
    
    return TimeSeriesData(
        series_id=series_id,
        title=series_info.get("title", f"FRED:{series_id}"),
        data=data_points,
        units=series_info.get("units", ""),
        frequency=series_info.get("frequency", "")
    )

@router.get("/ism/pmi", response_model=TimeSeriesData)
async def get_ism_pmi(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """
    Get ISM Manufacturing PMI data from DBNomics.
    """
    df = dbnom_service.get_ism_pmi(start_date, end_date)
    
    if df.empty:
        raise HTTPException(status_code=404, detail="No ISM PMI data found. Please check the DBNomics API.")
    
    # Convert to response model
    data_points = [
        TimeSeriesPoint(date=row["date"], value=row["value"]) 
        for _, row in df.iterrows() if pd.notna(row["value"])
    ]
    
    return TimeSeriesData(
        series_id="ISM-PMI",
        title="ISM Manufacturing PMI",
        data=data_points,
        units="Index",
        frequency="Monthly"
    )

@router.get("/ism/new-orders", response_model=TimeSeriesData)
async def get_ism_new_orders(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """
    Get ISM New Orders Index data from DBNomics.
    """
    df = dbnom_service.get_ism_new_orders(start_date, end_date)
    
    if df.empty:
        raise HTTPException(status_code=404, detail="No ISM New Orders data found. Please check the DBNomics API.")
    
    # Convert to response model
    data_points = [
        TimeSeriesPoint(date=row["date"], value=row["value"]) 
        for _, row in df.iterrows() if pd.notna(row["value"])
    ]
    
    return TimeSeriesData(
        series_id="ISM-NEW-ORDERS",
        title="ISM Manufacturing New Orders Index",
        data=data_points,
        units="Index",
        frequency="Monthly"
    )
    
@router.get("/dbnom/providers")
async def get_providers():
    """
    Get a list of all providers from DBNomics.
    """
    return dbnom_service.get_providers()

@router.get("/dbnom/datasets/{provider_code}")
async def get_datasets(
    provider_code: str = Path(..., description="Provider code (e.g., 'ISM')")
):
    """
    Get a list of all datasets for a provider from DBNomics.
    """
    return dbnom_service.get_datasets(provider_code)

@router.get("/dbnom/debug/{provider_code}/{dataset_code}/{series_code}")
async def debug_series(
    provider_code: str = Path(..., description="Provider code (e.g., 'ISM')"),
    dataset_code: str = Path(..., description="Dataset code (e.g., 'MAN_REPORT')"),
    series_code: str = Path(..., description="Series code (e.g., 'PMI')")
):
    """
    Debug endpoint to fetch a specific series by its components.
    """
    df = dbnom_service.get_series_by_id(provider_code, dataset_code, series_code)
    
    if df.empty:
        raise HTTPException(status_code=404, detail=f"No data found for {provider_code}/{dataset_code}/{series_code}")
    
    # Convert DataFrame to dict for JSON response
    result = {
        "series_id": f"{provider_code}/{dataset_code}/{series_code}",
        "data": df.to_dict(orient="records"),
        "total_observations": len(df)
    }
    
    return result

@router.get("/metals/gold-copper-ratio", response_model=TimeSeriesData)
async def get_gold_copper_ratio(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """
    Get Gold/Copper ratio data, a key indicator of economic sentiment.
    A rising ratio indicates risk-off sentiment (gold gaining vs copper), 
    while a falling ratio indicates risk-on sentiment (copper gaining vs gold).
    """
    df = composite_indicators_service.get_gold_copper_ratio(start_date, end_date)
    
    if df.empty:
        raise HTTPException(status_code=404, detail="No Gold/Copper ratio data found. Please check the API connection.")
    
    # Convert to response model
    data_points = [
        TimeSeriesPoint(date=row["date"], value=row["value"]) 
        for _, row in df.iterrows() if pd.notna(row["value"])
    ]
    
    return TimeSeriesData(
        series_id="GOLD-COPPER-RATIO",
        title="Gold/Copper Ratio",
        data=data_points,
        units="Ratio",
        frequency="Daily"
    )

@router.get("/market/sp500", response_model=TimeSeriesData)
async def get_sp500(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """
    Get S&P 500 index data.
    The S&P 500 is a stock market index tracking the stock performance of 500 large companies
    listed on stock exchanges in the United States.
    """
    df = composite_indicators_service.get_sp500_performance(start_date, end_date)
    
    if df.empty:
        raise HTTPException(status_code=404, detail="No S&P 500 data found. Please check the API connection.")
    
    # Convert to response model
    data_points = [
        TimeSeriesPoint(date=row["date"], value=row["value"]) 
        for _, row in df.iterrows() if pd.notna(row["value"])
    ]
    
    return TimeSeriesData(
        series_id="SP500",
        title="S&P 500 Index",
        data=data_points,
        units="Price",
        frequency="Daily"
    )

@router.get("/yahoo/{ticker}", response_model=TimeSeriesData)
async def get_yahoo_ticker(
    ticker: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """
    Get price data for any Yahoo Finance ticker.
    """
    df = yahoo_finance_service.get_ticker_data(ticker, start_date, end_date)
    
    if df.empty:
        raise HTTPException(status_code=404, detail=f"No data found for ticker {ticker}. Please check if the ticker symbol is valid.")
    
    # Convert to response model
    data_points = [
        TimeSeriesPoint(date=row["date"], value=row["value"]) 
        for _, row in df.iterrows() if pd.notna(row["value"])
    ]
    
    return TimeSeriesData(
        series_id=f"YAHOO-{ticker}",
        title=f"{ticker} Price",
        data=data_points,
        units="Price",
        frequency="Daily"
    )

=== backend\app\api\endpoints\new_indicators.py ===

# backend/app/api/endpoints/new_indicators.py

from fastapi import APIRouter, HTTPException, Query, Path
from typing import List, Optional
from app.services.unified_indicator_service import UnifiedIndicatorService
from app.models.indicators import (
    EnrichedIndicatorData,
    IndicatorMetadataResponse,
    CategoryInfo,
    MarketStatusResponse,
    IndicatorsByTypeResponse
)
from app.core.indicator_config import IndicatorType # For path parameter validation

router = APIRouter()
unified_service = UnifiedIndicatorService()

@router.get("/", response_model=List[IndicatorMetadataResponse])
async def get_all_indicators_metadata_list():
    """
    Get metadata for all available indicators.
    The order is determined by their definition in indicator_config.py.
    """
    try:
        # unified_service.get_all_indicators_metadata() now returns an ordered list
        return unified_service.get_all_indicators_metadata()
    except Exception as e:
        logger.error(f"Error fetching indicators metadata: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error fetching indicators metadata: {str(e)}")

@router.get("/categories", response_model=List[CategoryInfo])
async def get_categories():
    """
    Get all available categories with their defined indicators.
    Categories are sorted by 'display_order'. Indicators within each category
    are sorted by their definition order in indicator_config.py.
    """
    try:
        # unified_service.get_categories() now returns an ordered list of CategoryInfo
        return unified_service.get_categories()
    except Exception as e:
        logger.error(f"Error fetching categories: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error fetching categories: {str(e)}")

@router.get("/market-status", response_model=MarketStatusResponse)
async def get_market_status(
    indicators: Optional[str] = Query(None, description="Comma-separated list of indicator IDs to use")
):
    """
    Calculate overall market status based on multiple indicators.
    If 'indicators' query param is not provided, uses all indicators in their definition order.
    """
    try:
        indicator_list = None
        if indicators:
            indicator_list = [ind.strip() for ind in indicators.split(",")]

        return unified_service.calculate_market_status(indicator_list)
    except Exception as e:
        logger.error(f"Error calculating market status: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error calculating market status: {str(e)}")

@router.get("/type/{indicator_type_value}", response_model=IndicatorsByTypeResponse)
async def get_indicators_by_type_endpoint( # Renamed for clarity
    indicator_type_value: str = Path(..., description="The type of indicators to fetch (e.g., 'leading', 'coincident', 'lagging')"),
    start_date: Optional[str] = Query(None, description="Start date in YYYY-MM-DD format"),
    end_date: Optional[str] = Query(None, description="End date in YYYY-MM-DD format")
):
    """
    Get all indicators of a specific type (leading, coincident, or lagging)
    along with their data and relevant categories.
    Indicators and categories are ordered as defined in the configuration.
    """
    try:
        indicator_type_enum = IndicatorType(indicator_type_value.lower())
    except ValueError:
        valid_types = ", ".join([it.value for it in IndicatorType])
        logger.warning(f"Invalid indicator type requested: {indicator_type_value}")
        raise HTTPException(
            status_code=400,
            detail=f"Invalid indicator type '{indicator_type_value}'. Valid types are: {valid_types}."
        )
    
    try:
        # unified_service.get_enriched_indicators_by_type() returns an IndicatorsByTypeResponse
        # with ordered lists of indicators and categories.
        return unified_service.get_enriched_indicators_by_type(indicator_type_enum, start_date, end_date)
    except ValueError as e: 
        logger.error(f"Value error for indicator type '{indicator_type_value}': {e}", exc_info=True)
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Error fetching indicators by type '{indicator_type_value}': {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error fetching indicators by type '{indicator_type_value}': {str(e)}")


@router.get("/{indicator_id}", response_model=EnrichedIndicatorData)
async def get_indicator_data( 
    indicator_id: str,
    start_date: Optional[str] = Query(None, description="Start date in YYYY-MM-DD format"),
    end_date: Optional[str] = Query(None, description="End date in YYYY-MM-DD format")
):
    """
    Get enriched indicator data with transformations and signals for a single indicator.
    """
    try:
        return unified_service.get_indicator(indicator_id, start_date, end_date)
    except ValueError as e: 
        logger.warning(f"Indicator not found (ValueError): {indicator_id}, Error: {e}")
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Error fetching indicator {indicator_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error fetching indicator {indicator_id}: {str(e)}")

@router.get("/{indicator_id}/metadata", response_model=IndicatorMetadataResponse)
async def get_indicator_metadata_item( 
    indicator_id: str
):
    """
    Get metadata for a specific indicator.
    """
    try:
        return unified_service.get_indicator_metadata(indicator_id)
    except ValueError as e:
        logger.warning(f"Metadata not found for indicator (ValueError): {indicator_id}, Error: {e}")
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Error fetching metadata for {indicator_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error fetching metadata for {indicator_id}: {str(e)}")

@router.get("/categorydata/{category_name}", response_model=List[EnrichedIndicatorData]) # Changed path slightly for clarity
async def get_indicators_by_category_name_list( 
    category_name: str, 
    start_date: Optional[str] = Query(None, description="Start date in YYYY-MM-DD format"),
    end_date: Optional[str] = Query(None, description="End date in YYYY-MM-DD format")
):
    """
    Get all enriched indicators in a specific category by the category's name.
    Indicators are returned in their definition order from indicator_config.py.
    """
    try:
        # unified_service.get_indicators_by_category_name() returns an ordered list
        indicators = unified_service.get_indicators_by_category_name(category_name, start_date, end_date)
        # No specific error if category exists but has no indicators; an empty list is valid.
        return indicators
    except Exception as e:
        logger.error(f"Error fetching indicators for category '{category_name}': {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error fetching indicators for category '{category_name}': {str(e)}")

# Add logger to this file if not already present at the top
import logging
logger = logging.getLogger(__name__)


=== backend\app\core\__init__.py ===



=== backend\app\core\config.py ===

import os
from pydantic_settings import BaseSettings
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class Settings(BaseSettings):
    API_PORT: int = int(os.getenv("API_PORT", 8000))
    API_HOST: str = os.getenv("API_HOST", "0.0.0.0")
    PROJECT_NAME: str = "Macro Dashboard API"
    
    # External API keys
    FRED_API_KEY: str = os.getenv("FRED_API_KEY", "")
    
    # Database settings can be added later
    
    # CORS settings
    CORS_ORIGINS: list = ["*"]  # For development
    
    class Config:
        env_file = ".env"

settings = Settings()

=== backend\app\core\indicator_config.py ===

# backend/app/core/indicator_config.py

from typing import Dict, List, Optional, Union, Literal
from pydantic import BaseModel
from enum import Enum

class DataSourceType(str, Enum):
    FRED = "fred"
    YAHOO = "yahoo"
    DBNOMICS_ISM = "dbnomics_ism" # Specific for ISM data via DBNomics
    CUSTOM_COMPOSITE = "custom_composite" # For indicators calculated from multiple sources

class TransformationType(str, Enum):
    NONE = "none"
    YOY = "yoy"
    INVERT = "invert"

class SignalStatus(str, Enum):
    BULLISH = "bullish"
    BEARISH = "bearish"
    NEUTRAL = "neutral"

class IndicatorType(str, Enum):
    LEADING = "leading"
    COINCIDENT = "coincident"
    LAGGING = "lagging"

# --- Dynamic Threshold Configuration Models ---
class DynamicThresholdType(str, Enum):
    MOVING_AVERAGE_CROSSOVER = "moving_average_crossover"
    # Future types can be added here, e.g., BOLLINGER_BAND, PERCENTILE_RANK

class MovingAverageThresholdConfig(BaseModel):
    period: int
    ma_type: Literal["simple"] = "simple" # Could be extended to "ema" etc.

# Union for different dynamic threshold configurations
DynamicThresholdDetail = Union[MovingAverageThresholdConfig, None] # Add other config types to Union if needed

class DynamicThresholdConfig(BaseModel):
    type: DynamicThresholdType
    config: Optional[DynamicThresholdDetail] = None
# --- End Dynamic Threshold Configuration Models ---

class IndicatorMetadata(BaseModel):
    name: str
    category: str # This will be the category NAME, used to link to CategoryDefinition
    indicator_type: IndicatorType
    data_source: DataSourceType
    series_id: Optional[str] = None # For FRED, Yahoo
    # custom_endpoint: Optional[str] = None # Deprecated and removed
    
    # Static thresholds are still useful for many indicators or as fallbacks
    bullish_threshold: Optional[float] = None
    bearish_threshold: Optional[float] = None
    
    # New field for dynamic threshold logic
    dynamic_threshold: Optional[DynamicThresholdConfig] = None
    
    transformation: TransformationType
    description: Optional[str] = None
    y_axis_domain: Optional[List[float]] = None
    invert_logic: bool = False
    units: Optional[str] = None
    frequency: Optional[str] = None

class CategoryDefinition(BaseModel):
    id: str 
    name: str 
    description: str
    display_order: int

CATEGORY_DEFINITIONS_LIST: List[CategoryDefinition] = [
    CategoryDefinition(id="business-cycle-indicators", name="Business Cycle Indicators", display_order=1, description="Business cycle indicators track the expansion and contraction of the economy. ISM PMI and New Orders are particularly strong leading indicators with 3-9 month lead time."),
    CategoryDefinition(id="global-risk-metrics", name="Global Risk Metrics", display_order=2, description="Global risk metrics track investor sentiment and appetite for risk. The Gold/Copper ratio and VIX typically lead market turns by 3-6 months."),
    CategoryDefinition(id="financial-market-indicators", name="Financial Market Indicators", display_order=3, description="Financial market indicators help identify trends in equity and credit markets. The yield curve historically precedes recessions by 12-24 months."),
    CategoryDefinition(id="global-liquidity-metrics", name="Global Liquidity Metrics", display_order=4, description="Liquidity metrics track the availability of money and credit in the financial system. According to investors like Raoul Pal, liquidity drives everything in markets."),
    CategoryDefinition(id="housing-market", name="Housing Market", display_order=5, description="Housing market indicators are powerful leading indicators with 3-9 month forecast windows. Druckenmiller cites housing as a key sector to watch."),
    CategoryDefinition(id="market-sentiment", name="Market Sentiment", display_order=6, description="Market sentiment indicators track current investor mood and market participation, showing present-day risk appetite."),
    CategoryDefinition(id="economic-activity", name="Economic Activity", display_order=7, description="Economic activity indicators move simultaneously with the business cycle, reflecting current economic conditions in real-time."),
]

INDICATOR_DEFINITIONS: Dict[str, IndicatorMetadata] = {
    "ISM-PMI": IndicatorMetadata(
        name="ISM Manufacturing PMI",
        category="Business Cycle Indicators", indicator_type=IndicatorType.LEADING,
        data_source=DataSourceType.DBNOMICS_ISM, # series_id not needed if data_source implies it
        bullish_threshold=50.0, bearish_threshold=45.0, transformation=TransformationType.NONE,
        y_axis_domain=[30.0, 70.0], units="Index", frequency="Monthly",
        description="ISM Manufacturing PMI is a leading indicator with 3-6 month forecast window. Above 50 indicates expansion, below 45 indicates contraction."
    ),
    "ISM-NEW-ORDERS": IndicatorMetadata(
        name="ISM Manufacturing New Orders Index",
        category="Business Cycle Indicators", indicator_type=IndicatorType.LEADING,
        data_source=DataSourceType.DBNOMICS_ISM, # series_id not needed
        bullish_threshold=50.0, bearish_threshold=45.0, transformation=TransformationType.NONE,
        y_axis_domain=[30.0, 70.0], units="Index", frequency="Monthly",
        description="ISM New Orders Index is a leading indicator with 3-9 month forecast window. More forward-looking than PMI itself."
    ),
    "M2SL": IndicatorMetadata(
        name="M2 Money Supply",
        category="Global Liquidity Metrics", indicator_type=IndicatorType.LEADING,
        data_source=DataSourceType.FRED, series_id="M2SL",
        bullish_threshold=5.0, bearish_threshold=2.0, transformation=TransformationType.YOY,
        units="Percentage", frequency="Monthly",
        description="M2 Money Supply growth is a key liquidity indicator that precedes major market moves. When M2 growth exceeds 5% YoY, it typically creates a bullish environment for risk assets."
    ),
    "HOUST": IndicatorMetadata(
        name="Housing Starts",
        category="Housing Market", indicator_type=IndicatorType.LEADING,
        data_source=DataSourceType.FRED, series_id="HOUST",
        bullish_threshold=10.0, bearish_threshold=-10.0, transformation=TransformationType.YOY,
        units="Percentage", frequency="Monthly",
        description="Housing Starts are a leading indicator with a 3-6 month forecast window. YoY growth over 10% typically indicates an expanding economy."
    ),
    "PERMIT": IndicatorMetadata(
        name="Building Permits",
        category="Housing Market", indicator_type=IndicatorType.LEADING,
        data_source=DataSourceType.FRED, series_id="PERMIT",
        bullish_threshold=15.0, bearish_threshold=-5.0, transformation=TransformationType.YOY,
        units="Percentage", frequency="Monthly",
        description="Building Permits are an even earlier indicator than Housing Starts, with a 6-9 month lead time. They represent future construction activity."
    ),
    "T10Y2Y": IndicatorMetadata(
        name="Yield Curve (10Y-2Y)",
        category="Financial Market Indicators", indicator_type=IndicatorType.LEADING,
        data_source=DataSourceType.FRED, series_id="T10Y2Y",
        bullish_threshold=0.5, bearish_threshold=0.0, transformation=TransformationType.NONE,
        units="Percentage Points", frequency="Daily",
        description="The 10Y-2Y yield spread is a powerful predictor of recessions. When negative (inverted), it has historically preceded recessions by 12-24 months."
    ),
    "BAMLH0A0HYM2": IndicatorMetadata(
        name="Credit Spreads (High Yield)",
        category="Financial Market Indicators", indicator_type=IndicatorType.LEADING,
        data_source=DataSourceType.FRED, series_id="BAMLH0A0HYM2",
        bullish_threshold=4.0, bearish_threshold=6.0, transformation=TransformationType.NONE,
        invert_logic=True, units="Percentage Points", frequency="Daily",
        description="High-yield credit spreads measure risk appetite in credit markets and typically lead equity market moves by 1-3 months. Lower spreads indicate investor confidence."
    ),
    "VIX": IndicatorMetadata(
        name="VIX (Volatility Index)",
        category="Global Risk Metrics", indicator_type=IndicatorType.LEADING,
        data_source=DataSourceType.YAHOO, series_id="^VIX",
        bullish_threshold=20.0, bearish_threshold=30.0, transformation=TransformationType.NONE,
        invert_logic=True, y_axis_domain=[10.0, 50.0], units="Index", frequency="Daily",
        description="The VIX is a real-time market estimate of expected volatility. Lower values indicate risk-on sentiment, higher values indicate fear and risk-off sentiment."
    ),
    "GOLD-COPPER-RATIO": IndicatorMetadata(
        name="Gold/Copper Ratio",
        category="Global Risk Metrics", indicator_type=IndicatorType.LEADING,
        data_source=DataSourceType.CUSTOM_COMPOSITE, # series_id not applicable
        bullish_threshold=200.0, bearish_threshold=300.0, transformation=TransformationType.NONE,
        invert_logic=True, units="Ratio", frequency="Daily",
        description="The Gold/Copper ratio is a key market sentiment indicator. A falling ratio (copper outperforming gold) signals risk-on sentiment, while a rising ratio signals risk-off."
    ),
    "SP500": IndicatorMetadata(
        name="S&P 500 Index",
        category="Market Sentiment", indicator_type=IndicatorType.COINCIDENT,
        data_source=DataSourceType.YAHOO, series_id="^GSPC",
        # Static thresholds are placeholders; dynamic logic takes precedence
        bullish_threshold=0.0, bearish_threshold=0.0, 
        dynamic_threshold=DynamicThresholdConfig(
            type=DynamicThresholdType.MOVING_AVERAGE_CROSSOVER,
            config=MovingAverageThresholdConfig(period=125)
        ),
        transformation=TransformationType.NONE, units="Price", frequency="Daily",
        description="The S&P 500 index. Signal is dynamically determined by its position relative to the 125-day simple moving average (SMA). Above SMA is bullish, below is bearish."
    ),
    "INDPRO": IndicatorMetadata(
        name="Industrial Production Index",
        category="Economic Activity", indicator_type=IndicatorType.COINCIDENT,
        data_source=DataSourceType.FRED, series_id="INDPRO",
        bullish_threshold=2.0, bearish_threshold=-2.0, transformation=TransformationType.YOY,
        units="Percentage", frequency="Monthly",
        description="Industrial Production Index measures the real output of manufacturing, mining, and utilities. YoY growth above 2% typically indicates economic expansion."
    ),
}

# --- Helper functions ---
def get_indicator_metadata(indicator_id: str) -> Optional[IndicatorMetadata]:
    return INDICATOR_DEFINITIONS.get(indicator_id)

def get_all_indicators() -> Dict[str, IndicatorMetadata]:
    return INDICATOR_DEFINITIONS

def get_sorted_categories() -> List[CategoryDefinition]:
    return sorted(CATEGORY_DEFINITIONS_LIST, key=lambda cat: cat.display_order)

def get_category_by_name(name: str) -> Optional[CategoryDefinition]:
    for cat_def in CATEGORY_DEFINITIONS_LIST:
        if cat_def.name == name:
            return cat_def
    return None

def get_indicators_by_category_name(category_name: str) -> Dict[str, IndicatorMetadata]:
    return {
        indicator_id: metadata
        for indicator_id, metadata in INDICATOR_DEFINITIONS.items()
        if metadata.category == category_name
    }

def get_indicators_by_type(indicator_type: IndicatorType) -> Dict[str, IndicatorMetadata]:
    return {
        indicator_id: metadata
        for indicator_id, metadata in INDICATOR_DEFINITIONS.items()
        if metadata.indicator_type == indicator_type
    }



=== backend\app\db\__init__.py ===



=== backend\app\models\__init__.py ===



=== backend\app\models\indicators.py ===

# backend/app/models/indicators.py

from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime
from app.core.indicator_config import SignalStatus

class TimeSeriesPoint(BaseModel):
    date: datetime
    value: float

class TimeSeriesData(BaseModel):
    series_id: str
    title: str
    data: List[TimeSeriesPoint]
    units: Optional[str] = None
    frequency: Optional[str] = None

# Legacy model for backward compatibility
class IndicatorSignal(BaseModel):
    indicator_name: str
    indicator_value: float
    signal: str  # "bullish", "bearish", or "neutral"
    signal_threshold_bullish: float
    signal_threshold_bearish: float
    last_updated: datetime
    chart_data: List[TimeSeriesPoint]
    metadata: Optional[dict] = None

class EnrichedIndicatorData(BaseModel):
    """Enriched indicator data with transformations, signals, and metadata."""
    indicator_id: str
    title: str
    data: List[TimeSeriesPoint] # Primary data series (e.g., price, transformed value)
    units: Optional[str] = None
    frequency: Optional[str] = None
    category: str 
    description: Optional[str] = None
    bullish_threshold: float
    bearish_threshold: float
    signal_status: SignalStatus
    last_value: Optional[float] = None
    last_updated: Optional[datetime] = None
    y_axis_domain: Optional[List[float]] = None
    ma_series_data: Optional[List[TimeSeriesPoint]] = None # New field for MA line data

class IndicatorMetadataResponse(BaseModel):
    """Response model for indicator metadata."""
    indicator_id: str
    name: str
    category: str
    data_source: str
    description: Optional[str] = None
    units: Optional[str] = None
    frequency: Optional[str] = None

class CategoryInfo(BaseModel):
    """Information about an indicator category."""
    category_id: str 
    name: str
    description: str
    indicators: List[str]

class MarketStatusResponse(BaseModel):
    """Overall market status calculation."""
    bull_bear_status: str
    risk_on_off_status: str
    bull_bear_score: float
    risk_on_off_score: float
    total_indicators: int
    bullish_count: int
    bearish_count: int
    neutral_count: int
    last_updated: datetime

class IndicatorsByTypeResponse(BaseModel):
    """Response model for fetching indicators by their type (e.g., leading, coincident)."""
    indicator_type: str
    indicators: List[EnrichedIndicatorData]
    categories: List[CategoryInfo]


=== backend\app\services\__init__.py ===



=== backend\app\services\composite_indicators_service.py ===

# backend/app/services/composite_indicators_service.py

import pandas as pd
from datetime import datetime
import logging
from typing import Optional
from app.services.yahoo_finance_service import YahooFinanceService

logger = logging.getLogger(__name__)

class CompositeIndicatorsService:
    """Service for creating composite indicators from multiple data sources."""
    
    def __init__(self):
        """Initialize the composite indicators service."""
        self.yahoo_finance = YahooFinanceService()
        logger.info("CompositeIndicatorsService initialized")
    
    def get_gold_copper_ratio(self, start_date: Optional[str] = None, end_date: Optional[str] = None):
        """
        Calculate the gold/copper ratio, a key indicator of economic sentiment.
        A rising ratio indicates risk-off sentiment (gold gaining vs copper), 
        while a falling ratio indicates risk-on sentiment (copper gaining vs gold).
        
        Args:
            start_date (str, optional): Start date in YYYY-MM-DD format
            end_date (str, optional): End date in YYYY-MM-DD format
            
        Returns:
            pandas.DataFrame: DataFrame with date and ratio value columns
        """
        logger.info(f"Calculating gold/copper ratio with start_date={start_date}, end_date={end_date}")
        
        # Gold and copper tickers in Yahoo Finance
        gold_ticker = "GC=F"  # Gold Futures
        copper_ticker = "HG=F"  # Copper Futures
        
        try:
            # Get gold and copper prices using the Yahoo Finance service
            gold_df = self.yahoo_finance.get_ticker_data(gold_ticker, start_date, end_date)
            copper_df = self.yahoo_finance.get_ticker_data(copper_ticker, start_date, end_date)
            
            logger.info(f"Gold data shape: {gold_df.shape}, Copper data shape: {copper_df.shape}")
            
            if gold_df.empty:
                logger.warning("No gold price data returned from Yahoo Finance")
                return pd.DataFrame(columns=["date", "value"])
            
            if copper_df.empty:
                logger.warning("No copper price data returned from Yahoo Finance")
                return pd.DataFrame(columns=["date", "value"])
            
            # If we only have one row each, we can calculate a single ratio point
            if len(gold_df) == 1 and len(copper_df) == 1:
                logger.info("Calculating ratio from single data points")
                gold_price = gold_df.iloc[0]["value"]
                copper_price = copper_df.iloc[0]["value"]
                ratio = gold_price / copper_price
                
                logger.info(f"Gold price: {gold_price}, Copper price: {copper_price}, Ratio: {ratio}")
                
                return pd.DataFrame({
                    "date": [datetime.now()],
                    "value": [ratio]
                })
            
            # For multiple data points, merge on date and calculate ratio
            logger.info("Calculating ratio from multiple data points")
            
            # Merge the two DataFrames on date
            merged_df = pd.merge(gold_df, copper_df, on="date", suffixes=("_gold", "_copper"))
            
            # Calculate the ratio
            merged_df["value"] = merged_df["value_gold"] / merged_df["value_copper"]
            
            # Create the result DataFrame
            result_df = pd.DataFrame({
                "date": merged_df["date"],
                "value": merged_df["value"]
            })
            
            logger.info(f"Generated {len(result_df)} gold/copper ratio data points")
            if not result_df.empty:
                logger.debug(f"Ratio data sample: {result_df.head(3).to_dict('records')}")
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error calculating gold/copper ratio: {e}")
            import traceback
            traceback.print_exc()
            
            # Return empty DataFrame
            return pd.DataFrame(columns=["date", "value"])
    
    def get_sp500_performance(self, start_date: Optional[str] = None, end_date: Optional[str] = None):
        """
        Get S&P 500 index performance.
        
        Args:
            start_date (str, optional): Start date in YYYY-MM-DD format
            end_date (str, optional): End date in YYYY-MM-DD format
            
        Returns:
            pandas.DataFrame: DataFrame with date and value columns
        """
        logger.info(f"Fetching S&P 500 performance with start_date={start_date}, end_date={end_date}")
        
        # S&P 500 ticker in Yahoo Finance
        sp500_ticker = "^GSPC"
        
        try:
            # Get S&P 500 data directly from Yahoo Finance service
            sp500_df = self.yahoo_finance.get_ticker_data(sp500_ticker, start_date, end_date)
            
            if sp500_df.empty:
                logger.warning("No S&P 500 data returned from Yahoo Finance")
                return pd.DataFrame(columns=["date", "value"])
            
            logger.info(f"Retrieved {len(sp500_df)} S&P 500 data points")
            return sp500_df
            
        except Exception as e:
            logger.error(f"Error fetching S&P 500 data: {e}")
            import traceback
            traceback.print_exc()
            
            # Return empty DataFrame
            return pd.DataFrame(columns=["date", "value"])

=== backend\app\services\dbnom_service.py ===

import requests
import pandas as pd
from datetime import datetime, timedelta
import logging
from typing import Optional, List, Dict, Any

# Import for historical data
from app.services.historical_ism_data import get_historical_data as get_hardcoded_historical_ism_data

logger = logging.getLogger(__name__)

class DBNomicsService:
    """Service for fetching data from db.nomics.world API based on proper API structure."""

    BASE_URL = "https://api.db.nomics.world/v22"

    def __init__(self):
        pass

    def _merge_and_convert_to_df(
        self,
        historical_data_dicts: List[Dict[str, Any]], # Expects list of {'date': datetime, 'value': float}
        dbnomics_df_raw: pd.DataFrame, # Expects 'date' (datetime) and 'value' (float) columns
        requested_start_dt: Optional[datetime] = None,
        requested_end_dt: Optional[datetime] = None
    ) -> pd.DataFrame:
        """
        Merges historical data with DBNomics data. DBNomics data takes precedence for overlapping dates.
        Sorts, removes duplicates, and filters by the originally requested date range.
        """
        
        # Convert historical data dicts to DataFrame
        if historical_data_dicts:
            df_historical = pd.DataFrame(historical_data_dicts)
            df_historical['date'] = pd.to_datetime(df_historical['date'])
        else:
            df_historical = pd.DataFrame(columns=['date', 'value'])

        # Prepare DBNomics DataFrame
        if not dbnomics_df_raw.empty:
            df_dbnomics = dbnomics_df_raw.copy()
            df_dbnomics['date'] = pd.to_datetime(df_dbnomics['date'])
        else:
            df_dbnomics = pd.DataFrame(columns=['date', 'value'])

        # Concatenate historical data first, then DBNomics data
        # This way, if keep='last' is used for duplicates, DBNomics data is preferred
        combined_df = pd.concat([df_historical, df_dbnomics], ignore_index=True)
        
        if combined_df.empty:
            return pd.DataFrame(columns=["date", "value"])

        # Ensure date column is datetime
        combined_df['date'] = pd.to_datetime(combined_df['date'])
        
        # Sort by date
        combined_df.sort_values(by="date", inplace=True)
        
        # Drop duplicates on 'date', keeping the DBNomics version (the 'last' one for a given date)
        combined_df.drop_duplicates(subset=["date"], keep="last", inplace=True)

        # Final filtering based on the originally requested date range
        if requested_start_dt:
            combined_df = combined_df[combined_df["date"] >= requested_start_dt]
        if requested_end_dt:
            combined_df = combined_df[combined_df["date"] <= requested_end_dt]
            
        return combined_df.reset_index(drop=True)


    def get_series_by_id(self, provider_code: str, dataset_code: str, series_code: str, params: Optional[dict] = None, fetch_observations: bool = True):
        series_id = f"{provider_code}/{dataset_code}/{series_code}"
        endpoint = f"{self.BASE_URL}/series"

        query_params = {
            "series_ids": series_id,
        }

        if fetch_observations:
            query_params["observations"] = "true"
        
        # We will not pass start/end date params to DBNomics here,
        # as we want all available data from them to merge correctly with historical.
        # Date filtering will happen after merging.

        try:
            logger.info(f"Fetching ALL available series {series_id} from DBNomics with params: {query_params}")
            response = requests.get(endpoint, params=query_params)
            response.raise_for_status()
            data = response.json()

            if not data or "series" not in data or not isinstance(data.get("series"), dict) or "docs" not in data["series"]:
                logger.warning(f"No 'series.docs' key or expected structure in DBNomics response for {series_id}. Response: {data}")
                return pd.DataFrame(columns=["date", "value"])

            series_docs = data["series"]["docs"]

            if not isinstance(series_docs, list) or len(series_docs) == 0:
                logger.warning(f"'series.docs' is not a list or is empty for {series_id}. Full DBNomics response: {data}")
                return pd.DataFrame(columns=["date", "value"])

            series_data = series_docs[0]

            if fetch_observations: # This will always be true for our ISM calls
                if "period" not in series_data or "value" not in series_data:
                    logger.warning(f"Missing 'period' or 'value' in series data for {series_id}. Series data: {series_data}")
                    return pd.DataFrame(columns=["date", "value"])

                periods = series_data.get("period")
                values = series_data.get("value")

                if not isinstance(periods, list) or not isinstance(values, list) or len(periods) != len(values):
                    logger.warning(f"Period and value arrays have different lengths or are not lists for {series_id}")
                    return pd.DataFrame(columns=["date", "value"])

                if not periods:
                     logger.info(f"No observations (empty period list) returned for {series_id} from DBNomics.")
                     return pd.DataFrame(columns=["date", "value"])

                df = pd.DataFrame({
                    "date": pd.to_datetime(periods),
                    "value": pd.to_numeric(values, errors="coerce")
                })
                df = df.dropna(subset=['value'])
                
                logger.info(f"Successfully fetched {len(df)} observations for {series_id} from DBNomics.")
                return df
            else:
                # This path should ideally not be taken if fetch_observations is true.
                logger.info(f"Fetched metadata (no observations) for {series_id}")
                return pd.DataFrame(columns=["date", "value"])

        except requests.exceptions.RequestException as e:
            logger.error(f"Network error fetching series from DBNomics for {series_id}: {e}", exc_info=True)
            return pd.DataFrame(columns=["date", "value"])
        except ValueError as e: 
            logger.error(f"Error processing DBNomics response for {series_id}: {e}", exc_info=True)
            return pd.DataFrame(columns=["date", "value"])
        except Exception as e:
            logger.error(f"Unexpected error fetching series from DBNomics for {series_id}: {e}", exc_info=True)
            return pd.DataFrame(columns=["date", "value"])

    def _get_augmented_ism_series(
        self,
        provider: str,
        dataset: str,
        series: str,
        historical_series_key: str, 
        observation_start: Optional[str] = None,
        observation_end: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Fetches ISM series, augments with all relevant hardcoded historical data,
        prioritizes DBNomics data for overlaps, and then filters to the requested date range.
        """
        requested_start_dt = pd.to_datetime(observation_start) if observation_start else None
        requested_end_dt = pd.to_datetime(observation_end) if observation_end else datetime.now()

        # 1. Fetch ALL available historical data within the user's requested window.
        # If no start/end is given, historical_ism_data.py's helper will return all it has for the key.
        # For robustness, we pass the user's requested window to the historical data fetcher.
        historical_data_dicts: List[Dict[str, Any]] = []
        if requested_start_dt: # Only fetch historical if a start date is actually requested
            logger.info(f"Fetching hardcoded historical {historical_series_key} data from {requested_start_dt.strftime('%Y-%m-%d')} to {requested_end_dt.strftime('%Y-%m-%d')}")
            historical_data_dicts = get_hardcoded_historical_ism_data(
                historical_series_key,
                requested_start_dt.strftime("%Y-%m-%d"),
                requested_end_dt.strftime("%Y-%m-%d") 
            )
        else: # If no observation_start, we might not need very old historical data unless DBNomics is empty.
              # However, to ensure all PDF data is available if DBNomics fails or has gaps, fetch all historical.
              # The final filtering in _merge_and_convert_to_df will handle the range.
              # To be safe and ensure all provided PDF data is considered:
            logger.info(f"Fetching all available hardcoded historical {historical_series_key} data up to {requested_end_dt.strftime('%Y-%m-%d')}")
            # A very early start date to get all historical data up to requested_end_dt
            # This ensures if DBNomics is empty, we still get the full historical range.
            # The get_hardcoded_historical_ism_data will only return what's in the lists.
            earliest_possible_historical_start = "1900-01-01" # Or some other suitably early date
            historical_data_dicts = get_hardcoded_historical_ism_data(
                historical_series_key,
                earliest_possible_historical_start, # Fetch all historical data available
                requested_end_dt.strftime("%Y-%m-%d")
            )


        # 2. Fetch ALL available data from DBNomics (get_series_by_id fetches all, no date params passed)
        dbnomics_df_raw = self.get_series_by_id(provider, dataset, series, fetch_observations=True)
        
        # 3. Merge, prioritize DBNomics, and filter to the originally requested date range
        final_df = self._merge_and_convert_to_df(
            historical_data_dicts,
            dbnomics_df_raw,
            requested_start_dt, 
            requested_end_dt  
        )
        
        logger.info(f"Returning {len(final_df)} total points for {historical_series_key} after augmentation and final date filtering.")
        return final_df


    def get_ism_pmi(self, observation_start: Optional[str] = None, observation_end: Optional[str] = None):
        return self._get_augmented_ism_series(
            provider="ISM",
            dataset="pmi",
            series="pm",
            historical_series_key="PMI",
            observation_start=observation_start,
            observation_end=observation_end
        )

    def get_ism_new_orders(self, observation_start: Optional[str] = None, observation_end: Optional[str] = None):
        return self._get_augmented_ism_series(
            provider="ISM",
            dataset="neword", 
            series="in",      
            historical_series_key="NEW_ORDERS",
            observation_start=observation_start,
            observation_end=observation_end
        )

    # --- Other methods (get_providers, get_datasets) remain unchanged ---
    def get_providers(self):
        endpoint = f"{self.BASE_URL}/providers"
        try:
            logger.info("Fetching providers list from DBNomics")
            response = requests.get(endpoint)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"Network error fetching providers from DBNomics: {str(e)}", exc_info=True)
            return {}
        except ValueError as e: 
            logger.error(f"JSON decode error fetching providers from DBNomics: {str(e)}", exc_info=True)
            return {}

    def get_datasets(self, provider_code: str):
        endpoint = f"{self.BASE_URL}/datasets/{provider_code}"
        try:
            logger.info(f"Fetching datasets for provider {provider_code}")
            response = requests.get(endpoint)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"Network error fetching datasets for {provider_code} from DBNomics: {str(e)}", exc_info=True)
            return {}
        except ValueError as e: 
            logger.error(f"JSON decode error fetching datasets for {provider_code} from DBNomics: {str(e)}", exc_info=True)
            return {}


=== backend\app\services\fred_service.py ===

import requests
import pandas as pd
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)

class FredService:
    """Service for fetching data from the Federal Reserve Economic Data (FRED) API."""

    BASE_URL = "https://api.stlouisfed.org/fred"

    def __init__(self, api_key=None):
        self.api_key = api_key or settings.FRED_API_KEY
        if not self.api_key:
            logger.warning("FRED API key not provided. Service will not function properly.")

    def get_series_data(self, series_id, observation_start=None, observation_end=None):
        """
        Fetch time series data for a specific FRED series.
        Data will be fetched at its highest available frequency by default.

        Args:
            series_id (str): The FRED series ID (e.g., "UNRATE" for unemployment rate)
            observation_start (str, optional): Start date in YYYY-MM-DD format
            observation_end (str, optional): End date in YYYY-MM-DD format

        Returns:
            pandas.DataFrame: DataFrame with date and value columns
        """
        endpoint = f"{self.BASE_URL}/series/observations"

        params = {
            "series_id": series_id,
            "api_key": self.api_key,
            "file_type": "json",
            # "frequency": "m", # Removed to get highest available frequency
            "units": "lin",    # Levels (not percent change) - adjust if other units are needed
        }

        if observation_start:
            params["observation_start"] = observation_start
        if observation_end:
            params["observation_end"] = observation_end

        try:
            logger.info(f"Fetching FRED series {series_id} with params: {params}")
            response = requests.get(endpoint, params=params)
            response.raise_for_status() # Will raise an HTTPError for bad responses (4XX or 5XX)
            data = response.json()

            if "observations" in data and data["observations"]:
                df = pd.DataFrame(data["observations"])
                # Ensure 'date' is datetime and 'value' is numeric, handling potential FRED non-numeric values like '.'
                df["date"] = pd.to_datetime(df["date"])
                df["value"] = pd.to_numeric(df["value"], errors="coerce") # Coerce non-numeric to NaN
                df = df.dropna(subset=["value"]) # Remove rows where value became NaN
                
                logger.info(f"Successfully fetched {len(df)} observations for FRED series {series_id}")
                return df[["date", "value"]] # Ensure correct column order
            else:
                logger.warning(f"No data ('observations' key missing or empty) returned for FRED series {series_id}")
                return pd.DataFrame(columns=["date", "value"])

        except requests.exceptions.RequestException as e:
            logger.error(f"Network error fetching data from FRED for series {series_id}: {e}", exc_info=True)
            return pd.DataFrame(columns=["date", "value"])
        except ValueError as e: # Includes JSONDecodeError
            logger.error(f"Error decoding JSON or processing data for FRED series {series_id}: {e}", exc_info=True)
            return pd.DataFrame(columns=["date", "value"])
        except Exception as e:
            logger.error(f"Unexpected error fetching data from FRED for series {series_id}: {e}", exc_info=True)
            return pd.DataFrame(columns=["date", "value"])

    def get_series_info(self, series_id):
        """
        Get metadata about a specific FRED series.

        Args:
            series_id (str): The FRED series ID

        Returns:
            dict: Series metadata, or an empty dict if an error occurs or no info is found.
        """
        endpoint = f"{self.BASE_URL}/series"

        params = {
            "series_id": series_id,
            "api_key": self.api_key,
            "file_type": "json",
        }

        try:
            logger.info(f"Fetching series info for FRED series {series_id}")
            response = requests.get(endpoint, params=params)
            response.raise_for_status()
            data = response.json()

            if "seriess" in data and data["seriess"]: # Note: FRED API uses "seriess" for the list
                return data["seriess"][0]
            else:
                logger.warning(f"No info found ('seriess' key missing or empty) for FRED series {series_id}")
                return {}

        except requests.exceptions.RequestException as e:
            logger.error(f"Network error fetching series info from FRED for {series_id}: {e}", exc_info=True)
            return {}
        except ValueError as e: # Includes JSONDecodeError
            logger.error(f"Error decoding JSON for FRED series info {series_id}: {e}", exc_info=True)
            return {}
        except Exception as e:
            logger.error(f"Unexpected error fetching series info from FRED for {series_id}: {e}", exc_info=True)
            return {}



=== backend\app\services\historical_ism_data.py ===

# backend/app/services/historical_ism_data.py
import json
from datetime import datetime
import os
import logging

logger = logging.getLogger(__name__)

# Define the path to the JSON file relative to this file's location
# __file__ is the path to the current file (historical_ism_data.py)
# os.path.dirname(__file__) gets the directory of the current file
# os.path.join then creates a path like /path/to/services/historical_ism_data.json
JSON_DATA_FILE = os.path.join(os.path.dirname(__file__), "historical_ism_data.json")

_historical_data_cache = None

def _load_historical_data_from_json():
    """
    Loads historical ISM data from the JSON file.
    Caches the data after the first load to avoid repeated file I/O.
    """
    global _historical_data_cache
    if _historical_data_cache is not None:
        return _historical_data_cache

    try:
        with open(JSON_DATA_FILE, 'r') as f:
            data = json.load(f)
            # Data is loaded as is; date string conversion to datetime happens in get_historical_data
            _historical_data_cache = data
            logger.info(f"Successfully loaded historical ISM data from {JSON_DATA_FILE}")
            return data
    except FileNotFoundError:
        logger.error(f"Historical ISM data file not found: {JSON_DATA_FILE}")
        _historical_data_cache = {} # Cache empty dict to prevent repeated attempts
        return {}
    except json.JSONDecodeError:
        logger.error(f"Error decoding JSON from historical ISM data file: {JSON_DATA_FILE}", exc_info=True)
        _historical_data_cache = {}
        return {}
    except Exception as e:
        logger.error(f"An unexpected error occurred while loading historical ISM data: {e}", exc_info=True)
        _historical_data_cache = {}
        return {}

def get_historical_data(series_key: str, start_date_str: str, end_date_str: str):
    """
    Retrieves historical data for a given series key from the loaded JSON data,
    filtered within a specified date range.

    Args:
        series_key (str): Identifier for the series (e.g., "PMI", "NEW_ORDERS").
        start_date_str (str): Start date in 'YYYY-MM-DD' format.
        end_date_str (str): End date in 'YYYY-MM-DD' format.

    Returns:
        list: A list of {"date": datetime_object, "value": float} dicts.
    """
    all_historical_data = _load_historical_data_from_json() # Ensures data is loaded (and cached)

    if not all_historical_data or series_key not in all_historical_data:
        logger.warning(f"No historical data found for series key '{series_key}' in cache or file.")
        return []

    data_source = all_historical_data[series_key] # This is a list of {"date": "str", "value": float}

    results = []
    try:
        start_dt = datetime.strptime(start_date_str, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date_str, "%Y-%m-%d")
    except ValueError:
        logger.error(f"Invalid date format provided to get_historical_data: start='{start_date_str}', end='{end_date_str}'")
        return []

    for record in data_source:
        try:
            record_dt_str = record.get("date")
            record_value = record.get("value")

            if not isinstance(record_dt_str, str) or len(record_dt_str) != 10 or record_value is None:
                logger.warning(f"Skipping record with invalid format in JSON for {series_key}: {record}")
                continue
            
            record_dt = datetime.strptime(record_dt_str, "%Y-%m-%d")
            
            if start_dt <= record_dt <= end_dt:
                results.append({"date": record_dt, "value": float(record_value)})
        except ValueError: # Catches errors from strptime or float conversion
            logger.warning(f"Skipping record due to parsing error (date or value) for {series_key}: {record}")
            continue
        except TypeError: # Catches errors if record_value is not convertible to float
            logger.warning(f"Skipping record due to type error (likely for value) for {series_key}: {record}")
            continue

    results.sort(key=lambda x: x["date"])
    logger.debug(f"Returning {len(results)} historical records for {series_key} between {start_date_str} and {end_date_str}")
    return results



=== backend\app\services\indicator_processing_service.py ===

# backend/app/services/indicator_processing_service.py

import pandas as pd
from typing import List, Optional, Tuple
from datetime import datetime
import logging
from app.core.indicator_config import (
    IndicatorMetadata,
    TransformationType,
    SignalStatus,
    DynamicThresholdType, 
    MovingAverageThresholdConfig, 
    get_indicator_metadata
)
from app.models.indicators import TimeSeriesPoint, EnrichedIndicatorData 

logger = logging.getLogger(__name__)

class IndicatorProcessingService:
    """Service for processing and enriching indicator data with transformations and signals."""

    @staticmethod
    def calculate_yoy_growth(data: List[TimeSeriesPoint]) -> List[TimeSeriesPoint]:
        if not data or len(data) < 13:
            # logger.warning("Insufficient data for YoY calculation (need at least 13 points or data is empty)")
            return []
        
        df = pd.DataFrame([{"date": point.date, "value": point.value} for point in data])
        if df.empty:
            return []
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').set_index('date')
        
        df_resampled = df.resample('ME').last()
        df_resampled['yoy_growth'] = df_resampled['value'].pct_change(periods=12) * 100
        
        result = []
        for date, row in df_resampled.iterrows():
            if pd.notna(row['yoy_growth']):
                result.append(TimeSeriesPoint(date=date, value=round(row['yoy_growth'], 2)))
        
        # logger.info(f"Calculated YoY growth for {len(result)} data points")
        return result

    @staticmethod
    def calculate_moving_average(
        data: List[TimeSeriesPoint], 
        period: int,
        ma_type: str = "simple",
        indicator_id: Optional[str] = None # Added indicator_id as a parameter
    ) -> List[TimeSeriesPoint]:
        if not data or len(data) < period:
            logger.warning(f"IndicatorProcessingService ({indicator_id if indicator_id else 'Unknown'}): Insufficient data for {period}-period MA. Have {len(data)}, need {period}.")
            return []

        df = pd.DataFrame([{"date": point.date, "value": point.value} for point in data])
        if df.empty:
            return []
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date')

        if ma_type == "simple":
            df['ma'] = df['value'].rolling(window=period, min_periods=period).mean()
        else:
            logger.warning(f"Unsupported MA type: {ma_type} for indicator {indicator_id if indicator_id else 'Unknown'}. Defaulting to simple moving average.")
            df['ma'] = df['value'].rolling(window=period, min_periods=period).mean()
            
        result = []
        for _, row in df.iterrows():
            if pd.notna(row['ma']):
                result.append(TimeSeriesPoint(date=row['date'], value=round(row['ma'], 2)))
        
        logger.info(f"IndicatorProcessingService ({indicator_id if indicator_id else 'Unknown'}): Calculated {period}-period {ma_type} MA. Result length: {len(result)}. Input data length: {len(data)}")
        if indicator_id == 'SP500' and len(result) > 0:
            logger.debug(f"[SP500 MA DEBUG] First 3 MA points: {result[:3]}")
            logger.debug(f"[SP500 MA DEBUG] Last 3 MA points: {result[-3:]}")
        return result
    
    # Removed the class-level/module-level indicator_id_for_debug variable
    # indicator_id_for_debug: Optional[str] = None 


    @staticmethod
    def invert_values(data: List[TimeSeriesPoint]) -> List[TimeSeriesPoint]:
        if not data: return []
        result = [TimeSeriesPoint(date=point.date, value=-point.value) for point in data]
        # logger.info(f"Inverted values for {len(result)} data points")
        return result

    @staticmethod
    def apply_transformation(
        data: List[TimeSeriesPoint],
        transformation: TransformationType,
        metadata: IndicatorMetadata
    ) -> Tuple[List[TimeSeriesPoint], str, str]:
        transformed_data = data.copy() 
        title = metadata.name
        units = metadata.units or ""

        if not data: 
            # logger.warning(f"Cannot apply transformation for {metadata.name} as raw data is empty.")
            return [], title, units

        if transformation == TransformationType.YOY:
            yoy_data = IndicatorProcessingService.calculate_yoy_growth(data)
            if yoy_data: 
                transformed_data = yoy_data
                title = f"{metadata.name} (YoY % Change)"
                units = "% YoY"
            # else: 
                # logger.warning(f"YoY transformation failed for {metadata.name}, returning original data.")
        elif transformation == TransformationType.INVERT:
            transformed_data = IndicatorProcessingService.invert_values(data)
            title = f"{metadata.name} (Inverted)"
        
        return transformed_data, title, units

    @staticmethod
    def _determine_static_signal_status(
        value: Optional[float],
        bullish_threshold: Optional[float], 
        bearish_threshold: Optional[float], 
        invert_logic: bool = False
    ) -> SignalStatus:
        if value is None or bullish_threshold is None or bearish_threshold is None:
            return SignalStatus.NEUTRAL
        
        if invert_logic:
            if value <= bullish_threshold: return SignalStatus.BULLISH
            if value >= bearish_threshold: return SignalStatus.BEARISH
        else:
            if value >= bullish_threshold: return SignalStatus.BULLISH
            if value <= bearish_threshold: return SignalStatus.BEARISH
        return SignalStatus.NEUTRAL

    @staticmethod
    def _calculate_moving_average_crossover_signal(
        indicator_id: str, 
        data: List[TimeSeriesPoint], 
        ma_config: MovingAverageThresholdConfig
    ) -> Tuple[Optional[float], Optional[float], SignalStatus, Optional[float], List[TimeSeriesPoint]]:
        ma_series_data: List[TimeSeriesPoint] = [] 

        if not data or len(data) < ma_config.period:
            logger.warning(f"[{indicator_id}] Insufficient data for {ma_config.period}-period MA crossover signal. Data length: {len(data)}")
            return None, None, SignalStatus.NEUTRAL, None, ma_series_data

        # Pass indicator_id to calculate_moving_average for logging context
        ma_series_data = IndicatorProcessingService.calculate_moving_average(data, ma_config.period, ma_config.ma_type, indicator_id=indicator_id)
        
        if not ma_series_data: 
            logger.warning(f"[{indicator_id}] MA series calculation failed or resulted in empty list. Original data length: {len(data)}")
            return None, None, SignalStatus.NEUTRAL, None, [] 

        if not data: 
             logger.warning(f"[{indicator_id}] Original data is empty after MA calculation attempt, this should not happen.")
             return None, None, SignalStatus.NEUTRAL, None, ma_series_data


        last_price_point = data[-1]
        last_ma_point = ma_series_data[-1] 

        if last_ma_point is None or last_price_point is None: 
            logger.warning(f"[{indicator_id}] Could not get last price or MA value for crossover signal despite MA series being present.")
            return None, None, SignalStatus.NEUTRAL, None, ma_series_data

        last_value = last_price_point.value
        ma_value = last_ma_point.value
        
        signal = SignalStatus.NEUTRAL
        if last_value > ma_value:
            signal = SignalStatus.BULLISH
        elif last_value < ma_value:
            signal = SignalStatus.BEARISH
        
        logger.info(f"[{indicator_id}] MA Crossover: Last value {last_value}, MA({ma_config.period}) {ma_value}, Signal: {signal.value}. MA Series Length: {len(ma_series_data)}")
        if indicator_id == 'SP500':
            logger.debug(f"[SP500 _calculate_moving_average_crossover_signal] Returning MA series of length: {len(ma_series_data)}")
            if ma_series_data:
                 logger.debug(f"[SP500 _calculate_moving_average_crossover_signal] First 3 MA points returned: {ma_series_data[:3]}")

        return ma_value, ma_value, signal, ma_value, ma_series_data


    @staticmethod
    def process_indicator_data(
        indicator_id: str,
        raw_data: List[TimeSeriesPoint],
        original_title: str, 
        original_units: Optional[str] = None,
        original_frequency: Optional[str] = None
    ) -> EnrichedIndicatorData:
        metadata = get_indicator_metadata(indicator_id)
        if not metadata:
            logger.error(f"No metadata found for indicator {indicator_id}")
            return EnrichedIndicatorData(
                indicator_id=indicator_id, title=original_title, data=[],
                category="Unknown", bullish_threshold=0.0, bearish_threshold=0.0,
                signal_status=SignalStatus.NEUTRAL, description="Metadata not found."
            )
        
        if indicator_id == 'SP500':
            logger.debug(f"[SP500 process_indicator_data] Raw data length for SP500: {len(raw_data)}")
            if raw_data:
                logger.debug(f"[SP500 process_indicator_data] First 3 raw_data points for SP500: {raw_data[:3]}")


        display_data, processed_title, processed_units = IndicatorProcessingService.apply_transformation(
            raw_data.copy(), metadata.transformation, metadata
        )

        if not display_data: 
            logger.warning(f"No data after transformation for {indicator_id}. Using raw data for display if available, or empty.")
            display_data = raw_data if raw_data else []
            
        last_display_value: Optional[float] = None
        last_updated_date: Optional[datetime] = None
        if display_data:
            last_display_value = display_data[-1].value
            last_updated_date = display_data[-1].date

        bullish_thresh = metadata.bullish_threshold
        bearish_thresh = metadata.bearish_threshold
        signal_status = SignalStatus.NEUTRAL
        ma_series_for_response: Optional[List[TimeSeriesPoint]] = None 
        
        if metadata.dynamic_threshold:
            if indicator_id == 'SP500':
                logger.debug(f"[SP500 process_indicator_data] Processing dynamic threshold for SP500.")
            if metadata.dynamic_threshold.type == DynamicThresholdType.MOVING_AVERAGE_CROSSOVER:
                if isinstance(metadata.dynamic_threshold.config, MovingAverageThresholdConfig):
                    bt, brt, sig, ma_val, calculated_ma_series = IndicatorProcessingService._calculate_moving_average_crossover_signal(
                        indicator_id, 
                        raw_data, 
                        metadata.dynamic_threshold.config
                    )
                    bullish_thresh = ma_val 
                    bearish_thresh = ma_val
                    signal_status = sig
                    ma_series_for_response = calculated_ma_series 
                    if indicator_id == 'SP500':
                        logger.debug(f"[SP500 process_indicator_data] calculated_ma_series length: {len(calculated_ma_series if calculated_ma_series else [])}")
                        if ma_series_for_response:
                             logger.debug(f"[SP500 process_indicator_data] ma_series_for_response first 3: {ma_series_for_response[:3]}")
                else:
                    logger.error(f"Invalid config for MOVING_AVERAGE_CROSSOVER on {indicator_id}")
                    signal_status = IndicatorProcessingService._determine_static_signal_status(
                        last_display_value, bullish_thresh, bearish_thresh, metadata.invert_logic
                    )
        else: 
            signal_status = IndicatorProcessingService._determine_static_signal_status(
                last_display_value, bullish_thresh, bearish_thresh, metadata.invert_logic
            )
        
        if indicator_id == 'SP500':
            logger.debug(f"[SP500 process_indicator_data] Final ma_series_for_response length before return: {len(ma_series_for_response if ma_series_for_response else [])}")

        return EnrichedIndicatorData(
            indicator_id=indicator_id,
            title=processed_title, 
            data=display_data,    
            units=processed_units,
            frequency=metadata.frequency or original_frequency, 
            category=metadata.category,
            description=metadata.description,
            bullish_threshold=bullish_thresh if bullish_thresh is not None else 0.0, 
            bearish_threshold=bearish_thresh if bearish_thresh is not None else 0.0, 
            signal_status=signal_status,
            last_value=last_display_value, 
            last_updated=last_updated_date,
            y_axis_domain=metadata.y_axis_domain,
            ma_series_data=ma_series_for_response 
        )



=== backend\app\services\unified_indicator_service.py ===

# backend/app/services/unified_indicator_service.py

from typing import List, Optional, Dict, Set, Tuple
from datetime import datetime, timedelta
import logging
import pandas as pd
from app.core.indicator_config import (
    get_indicator_metadata,
    get_all_indicators,
    get_sorted_categories, 
    get_category_by_name,
    get_indicators_by_type,
    DataSourceType,
    TransformationType,
    IndicatorType,
    CategoryDefinition,
    SignalStatus 
)
from app.services.fred_service import FredService
from app.services.yahoo_finance_service import YahooFinanceService
from app.services.dbnom_service import DBNomicsService
from app.services.composite_indicators_service import CompositeIndicatorsService
from app.services.indicator_processing_service import IndicatorProcessingService
from app.models.indicators import (
    TimeSeriesPoint,
    EnrichedIndicatorData,
    IndicatorMetadataResponse,
    CategoryInfo, 
    MarketStatusResponse,
    IndicatorsByTypeResponse
)

logger = logging.getLogger(__name__)

class UnifiedIndicatorService:
    """Unified service for fetching, processing, and enriching indicator data with ordering."""

    def __init__(self):
        self.fred_service = FredService()
        self.yahoo_service = YahooFinanceService()
        self.dbnom_service = DBNomicsService()
        self.composite_service = CompositeIndicatorsService()
        self.processing_service = IndicatorProcessingService()

    def _adjust_start_date_for_transformation(
        self,
        start_date: Optional[str],
        transformation: TransformationType
    ) -> Optional[str]:
        # This function should correctly return the original start_date if transformation is not YOY
        # or if start_date is None initially.
        if not start_date or transformation != TransformationType.YOY:
            return start_date
        try:
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            adjusted_start_dt = start_dt - timedelta(days=365)
            return adjusted_start_dt.strftime('%Y-%m-%d')
        except ValueError:
            logger.warning(f"Invalid start_date format: {start_date}")
            return start_date

    def _fetch_raw_data(
        self,
        indicator_id: str,
        start_date: Optional[str] = None, # Parameter received from get_indicator
        end_date: Optional[str] = None   # Parameter received from get_indicator
    ) -> Tuple[List[TimeSeriesPoint], str, Optional[str], Optional[str]]:
        # --- ADDED DEBUG LOG ---
        logger.info(f"[_fetch_raw_data for {indicator_id}] Received params -> start_date: '{start_date}', end_date: '{end_date}'")
        
        metadata = get_indicator_metadata(indicator_id)
        if not metadata:
            logger.error(f"Metadata not found for indicator_id '{indicator_id}' during raw data fetch.")
            return [], f"Unknown Indicator: {indicator_id}", None, None

        # This is where adjusted_start_date is calculated
        adjusted_start_date = self._adjust_start_date_for_transformation(
            start_date, metadata.transformation
        )
        # The user's log showed this was None for SP500, implying 'start_date' input to _adjust_start_date_for_transformation was None
        logger.info(f"[_fetch_raw_data for {indicator_id}] Calculated adjusted_start_date: '{adjusted_start_date}' (original start_date was '{start_date}')")


        df = pd.DataFrame()
        data_points: List[TimeSeriesPoint] = []
        title = metadata.name 
        units = metadata.units
        frequency = metadata.frequency

        try:
            # When calling specific services, pass adjusted_start_date and the original end_date
            if metadata.data_source == DataSourceType.FRED:
                df = self.fred_service.get_series_data(metadata.series_id, adjusted_start_date, end_date)
                series_info = self.fred_service.get_series_info(metadata.series_id)
                title = series_info.get("title", metadata.name) 
                units = series_info.get("units", metadata.units)
                frequency = series_info.get("frequency", metadata.frequency)
            elif metadata.data_source == DataSourceType.YAHOO:
                # --- ADDED DEBUG LOG BEFORE YAHOO CALL ---
                logger.info(f"[_fetch_raw_data for {indicator_id}] Calling Yahoo with series_id: '{metadata.series_id}', start: '{adjusted_start_date}', end: '{end_date}'")
                df = self.yahoo_service.get_ticker_data(metadata.series_id, adjusted_start_date, end_date)
            elif metadata.data_source == DataSourceType.DBNOMICS_ISM:
                logger.info(f"[_fetch_raw_data for {indicator_id}] Calling DBNOMICS_ISM with start: '{adjusted_start_date}', end: '{end_date}'")
                if indicator_id == "ISM-PMI":
                    df = self.dbnom_service.get_ism_pmi(adjusted_start_date, end_date)
                elif indicator_id == "ISM-NEW-ORDERS":
                    df = self.dbnom_service.get_ism_new_orders(adjusted_start_date, end_date)
                else: 
                    logger.error(f"Unknown indicator ID: {indicator_id} for DataSourceType.DBNOMICS_ISM")
            elif metadata.data_source == DataSourceType.CUSTOM_COMPOSITE:
                logger.info(f"[_fetch_raw_data for {indicator_id}] Calling CUSTOM_COMPOSITE with start: '{adjusted_start_date}', end: '{end_date}'")
                if indicator_id == "GOLD-COPPER-RATIO":
                    df = self.composite_service.get_gold_copper_ratio(adjusted_start_date, end_date)
                else:
                    logger.error(f"Unknown CUSTOM_COMPOSITE indicator ID: {indicator_id}")
            else:
                logger.error(f"Unsupported data source: {metadata.data_source} for indicator {indicator_id}")

            if not df.empty:
                data_points = [
                    TimeSeriesPoint(date=row["date"], value=row["value"])
                    for _, row in df.iterrows() if pd.notna(row["value"])
                ]
            else:
                 logger.warning(f"No data returned from source for indicator {indicator_id}")
        except Exception as e:
            logger.error(f"Error fetching raw data for {indicator_id} from {metadata.data_source}: {e}", exc_info=True)
        
        logger.info(f"Fetched {len(data_points)} raw data points for {indicator_id}")
        return data_points, title, units, frequency

    def get_indicator(
        self,
        indicator_id: str,
        start_date: Optional[str] = None, # Parameter from get_enriched_indicators_by_type
        end_date: Optional[str] = None   # Parameter from get_enriched_indicators_by_type
    ) -> EnrichedIndicatorData:
        # --- ADDED DEBUG LOG ---
        logger.info(f"[get_indicator for {indicator_id}] Received params -> start_date: '{start_date}', end_date: '{end_date}'")
        
        metadata = get_indicator_metadata(indicator_id)
        if not metadata:
            logger.error(f"Metadata not found for indicator {indicator_id} in get_indicator.")
            return EnrichedIndicatorData(
                indicator_id=indicator_id, title=f"Unknown Indicator: {indicator_id}", data=[],
                category="Unknown", bullish_threshold=0.0, bearish_threshold=0.0,
                signal_status=SignalStatus.NEUTRAL, description="Metadata not found for this indicator."
            )

        # Pass received start_date and end_date to _fetch_raw_data
        raw_data, title, units, frequency = self._fetch_raw_data(indicator_id, start_date, end_date)
        
        enriched_data = self.processing_service.process_indicator_data(
            indicator_id, raw_data, title, units, frequency
        )
        return enriched_data

    def get_all_indicators_metadata(self) -> List[IndicatorMetadataResponse]:
        # ... (no changes here) ...
        all_indicators_meta = get_all_indicators() 
        response_list = []
        for indicator_id, metadata in all_indicators_meta.items():
            response_list.append(
                IndicatorMetadataResponse(
                    indicator_id=indicator_id, name=metadata.name, category=metadata.category, 
                    data_source=metadata.data_source.value, description=metadata.description,
                    units=metadata.units, frequency=metadata.frequency
                )
            )
        return response_list

    def get_categories(self) -> List[CategoryInfo]:
        # ... (no changes here) ...
        sorted_category_defs = get_sorted_categories() 
        all_indicators_meta = get_all_indicators()   
        category_info_list = []
        for cat_def in sorted_category_defs:
            indicator_ids_for_category = [
                indicator_id for indicator_id, meta in all_indicators_meta.items()
                if meta.category == cat_def.name 
            ]
            category_info_list.append(
                CategoryInfo(
                    category_id=cat_def.id, name=cat_def.name,
                    description=cat_def.description, indicators=indicator_ids_for_category
                )
            )
        return category_info_list
    
    def get_enriched_indicators_by_type(
        self,
        indicator_type: IndicatorType,
        start_date: Optional[str] = None, # Parameter from API endpoint
        end_date: Optional[str] = None   # Parameter from API endpoint
    ) -> IndicatorsByTypeResponse:
        # --- ADDED DEBUG LOG ---
        logger.info(f"[get_enriched_indicators_by_type for type '{indicator_type.value}'] Received params -> start_date: '{start_date}', end_date: '{end_date}'")
        
        typed_indicators_meta = get_indicators_by_type(indicator_type) 
        enriched_indicators_list: List[EnrichedIndicatorData] = []
        relevant_category_names_ordered: List[str] = [] 

        for indicator_id, metadata in typed_indicators_meta.items():
            try:
                # Pass the received start_date and end_date down
                enriched_data = self.get_indicator(indicator_id, start_date, end_date) 
                enriched_indicators_list.append(enriched_data)
                if metadata.category not in relevant_category_names_ordered:
                    relevant_category_names_ordered.append(metadata.category)
            except Exception as e:
                logger.error(f"Failed to fetch or process indicator {indicator_id} of type {indicator_type.value}: {e}", exc_info=True)
        
        final_categories_list: List[CategoryInfo] = []
        all_sorted_category_defs = get_sorted_categories() 
        for cat_def in all_sorted_category_defs:
            if cat_def.name in relevant_category_names_ordered:
                indicators_for_this_category_and_type = [
                    ind_id for ind_id, meta in typed_indicators_meta.items() if meta.category == cat_def.name
                ]
                if indicators_for_this_category_and_type:
                    final_categories_list.append(
                        CategoryInfo(
                            category_id=cat_def.id, name=cat_def.name,
                            description=cat_def.description, indicators=indicators_for_this_category_and_type 
                        )
                    )
        
        logger.info(f"Successfully fetched {len(enriched_indicators_list)} indicators of type {indicator_type.value}")
        return IndicatorsByTypeResponse(
            indicator_type=indicator_type.value, indicators=enriched_indicators_list, 
            categories=final_categories_list 
        )

    def get_indicators_by_category_name(self, category_name: str, start_date: Optional[str] = None, end_date: Optional[str] = None) -> List[EnrichedIndicatorData]:
        # ... (no changes here, but ensure it also passes dates correctly if used) ...
        logger.info(f"[get_indicators_by_category_name for '{category_name}'] Received params -> start_date: '{start_date}', end_date: '{end_date}'")
        indicators_meta_dict = {
            ind_id: meta for ind_id, meta in get_all_indicators().items() 
            if meta.category == category_name
        }
        results = []
        if not indicators_meta_dict:
            logger.warning(f"No indicators found defined for category name: {category_name}")
            return []
        for indicator_id in indicators_meta_dict.keys(): 
            try:
                indicator_data = self.get_indicator(indicator_id, start_date, end_date) # Pass dates
                results.append(indicator_data)
            except Exception as e:
                logger.error(f"Error fetching indicator {indicator_id} for category {category_name}: {e}", exc_info=True)
                continue
        return results

    def calculate_market_status(
        self,
        indicator_ids: Optional[List[str]] = None
    ) -> MarketStatusResponse:
        # ... (Market status logic - ensure get_indicator calls here also consider date ranges if needed for consistency, though typically it uses latest signal)
        # For simplicity, current market status uses get_indicator without specific dates, implying latest data.
        # If market status should reflect the dashboard's current view range, dates would need to be passed here too.
        # This part is unchanged for now as the primary issue is with the main data display.
        if indicator_ids is None:
            indicator_ids = list(get_all_indicators().keys())
        bullish_count = 0
        # ... (rest of the method)
        bearish_count = 0
        neutral_count = 0
        risk_on_count = 0 
        risk_off_count = 0

        logger.info(f"Calculating market status using {len(indicator_ids)} indicators")

        for indicator_id in indicator_ids: 
            try:
                # Market status typically uses the latest signal, so not passing dates here by default.
                indicator_data = self.get_indicator(indicator_id) 
                if indicator_data.signal_status == SignalStatus.BULLISH:
                    bullish_count += 1
                elif indicator_data.signal_status == SignalStatus.BEARISH:
                    bearish_count += 1
                else: 
                    neutral_count += 1
                
                metadata = get_indicator_metadata(indicator_id)
                if metadata and metadata.category in ["Global Risk Metrics", "Market Sentiment", "Financial Market Indicators"]:
                    if indicator_data.signal_status == SignalStatus.BULLISH:
                        risk_on_count +=1
                    elif indicator_data.signal_status == SignalStatus.BEARISH:
                         risk_off_count +=1
            except Exception as e:
                logger.error(f"Error processing indicator {indicator_id} for market status: {e}", exc_info=True)
                neutral_count += 1
                continue
        
        total_indicators = bullish_count + bearish_count + neutral_count
        bull_bear_status_val = "NEUTRAL"
        bull_bear_score_val = 50.0
        risk_on_off_status_val = "NEUTRAL"
        risk_on_off_score_val = 50.0

        if total_indicators > 0:
            bull_ratio = bullish_count / total_indicators
            bear_ratio = bearish_count / total_indicators

            if bull_ratio > 0.6 or (bullish_count > bearish_count and bull_ratio > 0.4):
                bull_bear_status_val = "BULL"
            elif bear_ratio > 0.6 or (bearish_count > bearish_count and bear_ratio > 0.4):
                bull_bear_status_val = "BEAR"
            
            bull_bear_score_val = 50 + (bullish_count - bearish_count) * (50 / total_indicators) 
            bull_bear_score_val = max(0, min(100, bull_bear_score_val))

            total_risk_relevant_indicators = risk_on_count + risk_off_count 
            if total_risk_relevant_indicators > 0:
                if risk_on_count > risk_off_count :
                    risk_on_off_status_val = "RISK-ON"
                elif risk_off_count > risk_on_count:
                    risk_on_off_status_val = "RISK-OFF"
                
                risk_on_off_score_val = 50 + (risk_on_count - risk_off_count) * (50 / total_risk_relevant_indicators)
                risk_on_off_score_val = max(0, min(100, risk_on_off_score_val))
            else: 
                risk_on_off_status_val = "NEUTRAL"
                risk_on_off_score_val = 50.0

        logger.info(f"Market status: {bull_bear_status_val} / {risk_on_off_status_val}")
        logger.info(f"Signal breakdown: {bullish_count} bullish, {bearish_count} bearish, {neutral_count} neutral")

        return MarketStatusResponse(
            bull_bear_status=bull_bear_status_val,
            risk_on_off_status=risk_on_off_status_val,
            bull_bear_score=bull_bear_score_val,
            risk_on_off_score=risk_on_off_score_val,
            total_indicators=total_indicators,
            bullish_count=bullish_count,
            bearish_count=bearish_count,
            neutral_count=neutral_count,
            last_updated=datetime.now()
        )


=== backend\app\services\yahoo_finance_service.py ===

# backend/app/services/yahoo_finance_service.py

import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import logging
import traceback

logger = logging.getLogger(__name__)

class YahooFinanceService:
    """Service for fetching price data using Yahoo Finance."""
    
    def __init__(self):
        """Initialize the Yahoo Finance service."""
        logger.info("YahooFinanceService initialized")
    
    def get_ticker_data(self, ticker_symbol: str, start_date=None, end_date=None):
        """
        Fetch ticker price data from Yahoo Finance.
        
        Args:
            ticker_symbol (str): The Yahoo Finance ticker symbol
            start_date (str, optional): Start date in YYYY-MM-DD format
            end_date (str, optional): End date in YYYY-MM-DD format
            
        Returns:
            pandas.DataFrame: DataFrame with date and value columns
        """
        logger.info(f"Fetching price data for ticker {ticker_symbol} from Yahoo Finance with start_date={start_date}, end_date={end_date}")
        
        try:
            # Convert string dates to datetime if provided
            start_dt = pd.to_datetime(start_date) if start_date else datetime.now() - timedelta(days=30)
            end_dt = pd.to_datetime(end_date) if end_date else datetime.now()
            
            # Fetch price data from Yahoo Finance
            logger.info(f"Fetching data for {ticker_symbol} from {start_dt} to {end_dt}")
            ticker_data = yf.download(ticker_symbol, start=start_dt, end=end_dt)
            
            # Check if data was returned
            if ticker_data.empty:
                logger.warning(f"No data returned from Yahoo Finance for {ticker_symbol}")
                return pd.DataFrame(columns=["date", "value"])
            
            # Log data sample for debugging at debug level
            logger.debug(f"Data shape: {ticker_data.shape}, columns: {ticker_data.columns.tolist()}")
            logger.debug(f"Data sample:\n{ticker_data.head(3).to_string()}")
            
            # Extract the closing prices and create a new DataFrame
            try:
                # Handle different column structures (could be multi-level)
                if isinstance(ticker_data.columns, pd.MultiIndex):
                    logger.debug(f"{ticker_symbol} data has multi-level columns")
                    close_cols = [col for col in ticker_data.columns if isinstance(col, tuple) and col[0] == 'Close']
                    if close_cols:
                        close_col = close_cols[0]
                    else:
                        logger.warning(f"Couldn't find 'Close' column in multi-level columns for {ticker_symbol}")
                        close_col = ticker_data.columns[0]  # Fall back to first column
                else:
                    close_col = 'Close'
                
                # Create a DataFrame with the date index and Close column value
                df = pd.DataFrame()
                df["date"] = ticker_data.index
                df["value"] = ticker_data[close_col].values
                
                logger.info(f"Successfully processed {len(df)} price data points for {ticker_symbol}")
                return df
                
            except Exception as e:
                logger.error(f"Error processing {ticker_symbol} data: {e}")
                traceback.print_exc()
                
                # Fallback: try a simpler approach with just the last data point
                logger.debug("Trying fallback method for ticker data")
                try:
                    latest_date = ticker_data.index[-1]
                    if isinstance(ticker_data.columns, pd.MultiIndex):
                        close_cols = [col for col in ticker_data.columns if isinstance(col, tuple) and col[0] == 'Close']
                        if close_cols:
                            latest_close = ticker_data.loc[latest_date, close_cols[0]]
                        else:
                            latest_close = ticker_data.iloc[-1, 0]  # Just get the first column's last value
                    else:
                        latest_close = ticker_data.loc[latest_date, 'Close']
                    
                    # Make sure we have a scalar value
                    if hasattr(latest_close, 'item'):
                        latest_close = latest_close.item()
                    
                    logger.debug(f"Fallback: Latest price at {latest_date}: {latest_close}")
                    
                    return pd.DataFrame({
                        "date": [latest_date],
                        "value": [latest_close]
                    })
                    
                except Exception as e2:
                    logger.error(f"Fallback method for {ticker_symbol} data failed: {e2}")
                    traceback.print_exc()
                    return pd.DataFrame(columns=["date", "value"])
            
        except Exception as e:
            logger.error(f"Error fetching price data for {ticker_symbol}: {e}")
            traceback.print_exc()
            return pd.DataFrame(columns=["date", "value"])

=== backend\main.py ===

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.exception_handlers import http_exception_handler
from starlette.exceptions import HTTPException as StarletteHTTPException
import logging
import argparse
import sys
from app.core.config import settings
from app.api.api import router as api_router

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

app = FastAPI(
    title=settings.PROJECT_NAME,
    description="API for the Macro Investment Dashboard",
    version="1.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global exception handler
@app.exception_handler(StarletteHTTPException)
async def custom_http_exception_handler(request: Request, exc: StarletteHTTPException):
    logger.error(f"HTTP error: {exc.detail}")
    return await http_exception_handler(request, exc)

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unexpected error: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error. Please try again later."}
    )

# Include routers
app.include_router(api_router, prefix="/api")

@app.get("/health")
async def health_check():
    """
    Health check endpoint to verify the API is running.
    """
    return {"status": "ok"}

@app.get("/")
async def root():
    """
    Root endpoint with API information.
    """
    return {
        "name": settings.PROJECT_NAME,
        "version": "1.0.0",
        "description": "API for the Macro Investment Dashboard",
        "endpoints": {
            "health": "/health",
            "api": "/api",
            "docs": "/docs"
        }
    }

def run_server(host: str = None, port: int = None, reload: bool = None):
    """
    Run the FastAPI server with specified parameters.
    
    Args:
        host: The host to bind to
        port: The port to bind to
        reload: Whether to reload on code changes
    """
    import uvicorn
    
    uvicorn_host = host or settings.API_HOST
    uvicorn_port = port or settings.API_PORT
    uvicorn_reload = reload if reload is not None else True
    
    logger.info(f"Starting server on {uvicorn_host}:{uvicorn_port} (reload={uvicorn_reload})")
    
    uvicorn.run(
        "main:app", 
        host=uvicorn_host, 
        port=uvicorn_port, 
        reload=uvicorn_reload
    )

if __name__ == "__main__":
    # Command line arguments for server configuration
    parser = argparse.ArgumentParser(description="Run the Macro Dashboard API server")
    parser.add_argument("--host", help="Host to bind to", default=None)
    parser.add_argument("--port", help="Port to bind to", type=int, default=None)
    parser.add_argument("--no-reload", help="Disable auto-reload on code changes", action="store_true")
    
    args = parser.parse_args()
    run_server(
        host=args.host, 
        port=args.port, 
        reload=not args.no_reload
    )

=== frontend\README.md ===

# Getting Started with Create React App

This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).

## Available Scripts

In the project directory, you can run:

### `npm start`

Runs the app in the development mode.\
Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

The page will reload if you make edits.\
You will also see any lint errors in the console.

### `npm test`

Launches the test runner in the interactive watch mode.\
See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.

### `npm run build`

Builds the app for production to the `build` folder.\
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.\
Your app is ready to be deployed!

See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.

### `npm run eject`

**Note: this is a one-way operation. Once you `eject`, you can’t go back!**

If you aren’t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.

Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.

You don’t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.

## Learn More

You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).

To learn React, check out the [React documentation](https://reactjs.org/).


=== frontend\public\index.html ===

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Web site created using create-react-app"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>React App</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>


=== frontend\src\App.css ===

.App {
  text-align: center;
}

.App-logo {
  height: 40vmin;
  pointer-events: none;
}

@media (prefers-reduced-motion: no-preference) {
  .App-logo {
    animation: App-logo-spin infinite 20s linear;
  }
}

.App-header {
  background-color: #282c34;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  font-size: calc(10px + 2vmin);
  color: white;
}

.App-link {
  color: #61dafb;
}

@keyframes App-logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}


=== frontend\src\App.test.tsx ===

import React from 'react';
import { render, screen } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  render(<App />);
  const linkElement = screen.getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});


=== frontend\src\App.tsx ===

import React from 'react';
import { BrowserRouter as Router, Routes, Route } from 'react-router-dom';
import { ThemeProvider, createTheme, CssBaseline } from '@mui/material';
import Dashboard from './pages/Dashboard';
import { IndicatorProvider } from './context/IndicatorContext';
import './App.css';

// Create a theme
const theme = createTheme({
  palette: {
    mode: 'dark',
    primary: {
      main: '#90caf9',
    },
    secondary: {
      main: '#f48fb1',
    },
  },
});

function App() {
  return (
    <ThemeProvider theme={theme}>
      <CssBaseline />
      <IndicatorProvider>
        <Router>
          <Routes>
            <Route path="/" element={<Dashboard />} />
            {/* Add more routes here later */}
          </Routes>
        </Router>
      </IndicatorProvider>
    </ThemeProvider>
  );
}

export default App;

=== frontend\src\components\CategoryHeader.tsx ===

import React from 'react';
import { Paper, Typography } from '@mui/material';

interface CategoryHeaderProps {
  id: string;
  title: string;
  description: string;
}

/**
 * A component that renders a category header with a title and description
 */
const CategoryHeader: React.FC<CategoryHeaderProps> = ({ id, title, description }) => {
  return (
    <Paper 
      id={id} 
      sx={{ 
        p: 3, 
        mb: 2, 
        bgcolor: 'primary.dark', 
        color: 'white', 
        borderRadius: 2,
        scrollMarginTop: '80px' 
      }}
    >
      <Typography variant="h5" gutterBottom fontWeight="bold">
        {title}
      </Typography>
      <Typography variant="body1">
        {description}
      </Typography>
    </Paper>
  );
};

export default CategoryHeader;

=== frontend\src\components\CategoryNavigation.tsx ===

import React from 'react';
import { Box, Chip, Link, Typography } from '@mui/material';

export interface Category {
  id: string;
  name: string;
}

interface CategoryNavigationProps {
  categories: Category[];
}

/**
 * A component that provides navigation between different categories using quick jump chips
 */
const CategoryNavigation: React.FC<CategoryNavigationProps> = ({ categories }) => {
  // Render category quick links as chips
  const renderQuickLinks = () => {
    return (
      <Box sx={{ display: 'flex', flexWrap: 'wrap', gap: 1, mb: 3 }}>
        <Typography variant="body2" sx={{ mr: 1, alignSelf: 'center' }}>
          Quick Jump:
        </Typography>
        {categories.map((category) => (
          <Chip
            key={category.id}
            label={category.name}
            component={Link}
            href={`#${category.id}`}
            onClick={(e) => {
              e.preventDefault();
              const element = document.getElementById(category.id);
              if (element) {
                element.scrollIntoView({ behavior: 'smooth' });
              }
            }}
            sx={{ cursor: 'pointer' }}
            clickable
          />
        ))}
      </Box>
    );
  };

  return renderQuickLinks();
};

export default CategoryNavigation;

=== frontend\src\components\CoincidentIndicators.tsx ===

import React, { useEffect, useState, useRef, useCallback, useMemo } from 'react';
import { 
  Typography, 
  Box, 
  Button
} from '@mui/material';
import { IndicatorData } from './IndicatorCard';
import IndicatorService from '../services/IndicatorService';
import { useIndicators } from '../context/IndicatorContext';
import CategoryHeader from './CategoryHeader';
import CategoryNavigation from './CategoryNavigation';
import IndicatorSection from './IndicatorSection';

const CoincidentIndicators: React.FC = () => {
  const { setIndicator } = useIndicators();
  const setIndicatorRef = useRef(setIndicator);
  
  // Update the ref when setIndicator changes
  useEffect(() => {
    setIndicatorRef.current = setIndicator;
  }, [setIndicator]);
  
  // State for S&P 500
  const [sp500Data, setSp500Data] = useState<IndicatorData | null>(null);
  const [sp500Loading, setSp500Loading] = useState<boolean>(true);
  const [sp500Error, setSp500Error] = useState<string | null>(null);
  
  // FRED series states for coincident indicators
  const [fredSeriesStates, setFredSeriesStates] = useState<{
    [seriesId: string]: {
      data: IndicatorData | null;
      loading: boolean;
      error: string | null;
    }
  }>({});
  
  // Flag to prevent automatic re-fetching
  const [hasInitiallyLoaded, setHasInitiallyLoaded] = useState<boolean>(false);
  
  // Define coincident indicators
  const coincidentIndicatorSeries = useMemo(() => [
    'INDPRO'  // Industrial Production Index
  ], []);
  
  // Categories for coincident indicators
  const categories = useMemo(() => [
    { id: 'market-sentiment', name: 'Market Sentiment' },
    { id: 'economic-activity', name: 'Economic Activity' }
  ], []);
  
  // Category descriptions
  const categoryDescriptions = useMemo(() => ({
    'economic-activity': 'Economic activity indicators move simultaneously with the business cycle, reflecting current economic conditions in real-time.',
    'market-sentiment': 'Market sentiment indicators track current investor mood and market participation, showing present-day risk appetite.'
  }), []);
  
  // Calculate the date range (last 4 years)
  const dateRange = useMemo(() => {
    const endDate = new Date();
    const formattedEndDate = endDate.toISOString().split('T')[0];
    
    const startDate = new Date();
    startDate.setFullYear(endDate.getFullYear() - 4);
    const formattedStartDate = startDate.toISOString().split('T')[0];
    
    return {
      startDate: formattedStartDate,
      endDate: formattedEndDate
    };
  }, []);

  // Fetch indicators on component mount
  useEffect(() => {
    if (!hasInitiallyLoaded) {
      const fetchCoincidentIndicators = async () => {
        
        // Fetch S&P 500
        try {
          setSp500Loading(true);
          const data = await IndicatorService.getSP500Data(dateRange.startDate, dateRange.endDate);
          setSp500Data(data);
          setIndicatorRef.current('SP500', data);
          setSp500Error(null);
        } catch (error) {
          console.error('Error fetching S&P 500 data:', error);
          setSp500Error('Failed to load S&P 500 data');
        } finally {
          setSp500Loading(false);
        }
        
        // Initialize FRED series states
        const initialFredStates = {} as any;
        coincidentIndicatorSeries.forEach(seriesId => {
          initialFredStates[seriesId] = {
            data: null,
            loading: true,
            error: null
          };
        });
        setFredSeriesStates(initialFredStates);
        
        // Fetch FRED series in parallel
        const fredPromises = coincidentIndicatorSeries.map(async (seriesId) => {
          try {
            const data = await IndicatorService.getFREDData(seriesId, dateRange.startDate, dateRange.endDate);
            setIndicatorRef.current(`FRED-${seriesId}`, data);
            return { seriesId, data, error: null };
          } catch (error) {
            console.error(`Error fetching FRED data for ${seriesId}:`, error);
            return { seriesId, data: null, error: `Failed to load ${seriesId} data from FRED` };
          }
        });
        
        // Update states for all FRED series
        const results = await Promise.all(fredPromises);
        const newFredStates = { ...initialFredStates };
        
        results.forEach(({ seriesId, data, error }) => {
          newFredStates[seriesId] = {
            data,
            loading: false,
            error
          };
        });
        
        setFredSeriesStates(newFredStates);
        setHasInitiallyLoaded(true);
      };
      
      fetchCoincidentIndicators();
    }
  }, [dateRange, coincidentIndicatorSeries, hasInitiallyLoaded]);
  
  // Refresh all data
  const handleRefreshAll = useCallback(async () => {
    setSp500Loading(true);
    
    // Update FRED loading states
    const updatedFredStates = { ...fredSeriesStates };
    Object.keys(updatedFredStates).forEach(seriesId => {
      updatedFredStates[seriesId] = {
        ...updatedFredStates[seriesId],
        loading: true
      };
    });
    setFredSeriesStates(updatedFredStates);
    
    // Refresh S&P 500
    try {
      const data = await IndicatorService.getSP500Data(dateRange.startDate, dateRange.endDate);
      setSp500Data(data);
      setIndicatorRef.current('SP500', data);
      setSp500Error(null);
    } catch (error) {
      console.error('Error refreshing S&P 500 data:', error);
      setSp500Error('Failed to refresh S&P 500 data');
    } finally {
      setSp500Loading(false);
    }
    
    // Refresh FRED series
    const fredPromises = coincidentIndicatorSeries.map(async (seriesId) => {
      try {
        const data = await IndicatorService.getFREDData(seriesId, dateRange.startDate, dateRange.endDate);
        setIndicatorRef.current(`FRED-${seriesId}`, data);
        return { seriesId, data, error: null };
      } catch (error) {
        console.error(`Error refreshing FRED data for ${seriesId}:`, error);
        return { seriesId, data: null, error: `Failed to refresh ${seriesId} data from FRED` };
      }
    });
    
    const results = await Promise.all(fredPromises);
    const newFredStates = { ...fredSeriesStates };
    
    results.forEach(({ seriesId, data, error }) => {
      newFredStates[seriesId] = {
        data,
        loading: false,
        error
      };
    });
    
    setFredSeriesStates(newFredStates);
  }, [dateRange, fredSeriesStates, coincidentIndicatorSeries]);

  // Get the most recent date from all indicators
  const mostRecentUpdateDate = useMemo(() => {
    let dates: Date[] = [];
    
    if (sp500Data?.data && sp500Data.data.length > 0) {
      dates.push(new Date(sp500Data.data[sp500Data.data.length - 1].date));
    }
    
    Object.values(fredSeriesStates).forEach(({ data }) => {
      if (data?.data && data.data.length > 0) {
        dates.push(new Date(data.data[data.data.length - 1].date));
      }
    });
    
    if (dates.length === 0) {
      return 'No data available';
    }
    
    const mostRecentDate = new Date(Math.max(...dates.map(date => date.getTime())));
    
    return mostRecentDate.toLocaleDateString('en-US', {
      year: 'numeric',
      month: 'long',
      day: 'numeric'
    });
  }, [sp500Data, fredSeriesStates]);

  return (
    <Box>
      <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', mb: 3 }}>
        <Typography variant="h5" component="h2">
          Coincident Indicators
        </Typography>
        <Button 
          variant="contained" 
          color="primary" 
          onClick={handleRefreshAll}
        >
          Refresh All Data
        </Button>
      </Box>
      
      {/* Category Navigation */}
      <CategoryNavigation categories={categories} />
      
      {/* Market Sentiment */}
      <Box id="market-sentiment" sx={{ mb: 4, scrollMarginTop: '80px' }}>
        <CategoryHeader 
          id="market-sentiment"
          title="Market Sentiment"
          description={categoryDescriptions['market-sentiment']}
        />
        
        {/* S&P 500 */}
        <IndicatorSection
          title="SP500"
          data={sp500Data}
          isLoading={sp500Loading}
          error={sp500Error}
          defaultTitle="S&P 500 Index"
        />
      </Box>
      
      {/* Economic Activity */}
      <Box id="economic-activity" sx={{ mb: 4, scrollMarginTop: '80px' }}>
        <CategoryHeader 
          id="economic-activity"
          title="Economic Activity"
          description={categoryDescriptions['economic-activity']}
        />
        
        {/* Industrial Production Index */}
        <IndicatorSection
          title="INDPRO"
          data={fredSeriesStates['INDPRO']?.data || null}
          isLoading={fredSeriesStates['INDPRO']?.loading || false}
          error={fredSeriesStates['INDPRO']?.error || null}
          defaultTitle="Industrial Production Index"
        />
      </Box>
      
      <Box sx={{ mt: 4, textAlign: 'right' }}>
        <Typography variant="caption" color="text.secondary">
          Last fetched: {hasInitiallyLoaded ? new Date().toLocaleString() : 'Loading...'}
        </Typography>
        <Typography variant="caption" color="text.secondary" display="block">
          Most recent data point: {mostRecentUpdateDate}
        </Typography>
      </Box>
    </Box>
  );
};

export default CoincidentIndicators;

=== frontend\src\components\IndicatorCard.tsx ===

import React from 'react';
import {
  Box,
  Card,
  CardContent,
  Typography,
  Chip,
  CircularProgress,
  Divider
} from '@mui/material';
import {
  TrendingUp as TrendingUpIcon,
  TrendingDown as TrendingDownIcon,
  Help as HelpIcon
} from '@mui/icons-material';
import {
  LineChart,
  Line,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  ResponsiveContainer,
  ReferenceLine,
  TooltipProps
  // Legend // Removed Legend import
} from 'recharts';
import { NameType, ValueType } from 'recharts/types/component/DefaultTooltipContent';

// Type definitions
export interface TimeSeriesPoint {
  date: string; 
  value: number;
}

export interface IndicatorData {
  series_id: string;
  title: string;
  data: TimeSeriesPoint[]; // Primary data series
  units?: string;
  frequency?: string;
  lastValue?: number;
  bullishThreshold?: number;
  bearishThreshold?: number;
  signalStatus?: 'bullish' | 'bearish' | 'neutral';
  yAxisDomain?: [number, number];
  description?: string; 
  ma_series_data?: TimeSeriesPoint[]; 
}

export type SignalStatus = 'bullish' | 'bearish' | 'neutral';

interface IndicatorCardProps {
  indicator: IndicatorData;
  isLoading?: boolean;
  error?: string | null;
}

const formatDateDutch = (date: Date): string => {
  return date.toLocaleDateString('nl-NL', {
    day: '2-digit',
    month: '2-digit',
    year: 'numeric',
  });
};

const CustomTooltipContent: React.FC<TooltipProps<ValueType, NameType> & { indicatorTitle: string, indicatorUnits?: string, maLabel?: string }> = ({ active, payload, indicatorTitle, indicatorUnits, maLabel }) => {
  if (active && payload && payload.length) {
    const dataPoint = payload[0].payload as { timestamp: number; originalDate: Date; value: number; maValue?: number }; 
    if (!dataPoint) return null;

    return (
      <Box sx={{
        backgroundColor: 'background.paper', 
        padding: '10px',
        border: '1px solid',
        borderColor: 'divider', 
        borderRadius: '4px',
        boxShadow: '0 2px 5px rgba(0,0,0,0.15)',
        color: 'text.primary' 
      }}>
        <Typography variant="caption" display="block" sx={{ fontWeight: 'bold', mb: 0.5 }}>
          {indicatorTitle}
        </Typography>
        <Typography variant="caption" display="block">
          Date: {formatDateDutch(dataPoint.originalDate)}
        </Typography>
        <Typography variant="caption" display="block">
          {indicatorUnits ? `${indicatorUnits}: ` : 'Value: '}
          {typeof dataPoint.value === 'number' ? dataPoint.value.toFixed(2) : 'N/A'}
        </Typography>
        {/* Display MA value in tooltip if present and is a number */}
        {payload.find(p => p.dataKey === 'maValue') && typeof dataPoint.maValue === 'number' && (
          <Typography variant="caption" display="block" sx={{color: '#ff7300'}}>
            {maLabel || 'MA'}: {dataPoint.maValue.toFixed(2)}
          </Typography>
        )}
      </Box>
    );
  }
  return null;
};


const IndicatorCard: React.FC<IndicatorCardProps> = ({
  indicator,
  isLoading = false,
  error = null
}) => {
  // --- DEBUGGING LOGS START ---
  if (indicator.series_id === 'SP500') { // Log only for S&P 500 to avoid console spam
    console.log(`[IndicatorCard SP500] Received indicator prop:`, JSON.parse(JSON.stringify(indicator)));
    if (indicator.ma_series_data) {
      console.log(`[IndicatorCard SP500] ma_series_data (length ${indicator.ma_series_data.length}):`, JSON.parse(JSON.stringify(indicator.ma_series_data.slice(0, 5)))); // Log first 5 MA points
    } else {
      console.log(`[IndicatorCard SP500] ma_series_data is undefined or empty.`);
    }
  }
  // --- DEBUGGING LOGS END ---

  const chartData = React.useMemo(() =>
    indicator.data?.map(point => {
      const dateObj = new Date(point.date);
      return {
        timestamp: dateObj.getTime(),
        originalDate: dateObj,
        value: point.value
      };
    }).sort((a, b) => a.timestamp - b.timestamp) || []
  , [indicator.data]);

  const combinedChartData = React.useMemo(() => {
    if (!indicator.ma_series_data || indicator.ma_series_data.length === 0) {
      return chartData.map(p => ({ ...p, maValue: undefined })); 
    }

    const maDataMap = new Map<number, number>();
    indicator.ma_series_data.forEach(point => {
      if (point && typeof point.date === 'string' && typeof point.value === 'number') { 
        maDataMap.set(new Date(point.date).getTime(), point.value);
      }
    });

    return chartData.map(primaryPoint => ({
      ...primaryPoint,
      maValue: maDataMap.get(primaryPoint.timestamp) 
    }));
  }, [chartData, indicator.ma_series_data]);

  // --- DEBUGGING LOGS START ---
  if (indicator.series_id === 'SP500') {
    console.log(`[IndicatorCard SP500] combinedChartData (length ${combinedChartData.length}):`, JSON.parse(JSON.stringify(combinedChartData.slice(0, 5)))); // Log first 5 combined points
    const pointsWithMaValue = combinedChartData.filter(p => typeof p.maValue === 'number');
    console.log(`[IndicatorCard SP500] Number of points with actual maValue: ${pointsWithMaValue.length}`);
    if (pointsWithMaValue.length > 0) {
        console.log(`[IndicatorCard SP500] First few points with maValue:`, JSON.parse(JSON.stringify(pointsWithMaValue.slice(0,5))));
    }
  }
  // --- DEBUGGING LOGS END ---


  const lastValue = indicator.lastValue ?? (chartData.length > 0 ? chartData[chartData.length - 1].value : undefined);

  const signalStatus = indicator.signalStatus || ((): SignalStatus => {
    if (lastValue === undefined || indicator.bullishThreshold === undefined || indicator.bearishThreshold === undefined) {
      return 'neutral';
    }
    if (indicator.bullishThreshold < indicator.bearishThreshold) { 
        if (lastValue <= indicator.bullishThreshold) return 'bullish';
        if (lastValue >= indicator.bearishThreshold) return 'bearish';
    } else { 
        if (lastValue >= indicator.bullishThreshold) return 'bullish';
        if (lastValue <= indicator.bearishThreshold) return 'bearish';
    }
    return 'neutral';
  })();

  const getSignalColor = (status: SignalStatus): string => {
    switch (status) {
      case 'bullish': return '#4caf50'; 
      case 'bearish': return '#f44336'; 
      default: return '#ff9800'; 
    }
  };

  const getSignalIcon = (status: SignalStatus) => {
    switch (status) {
      case 'bullish': return <TrendingUpIcon />;
      case 'bearish': return <TrendingDownIcon />;
      default: return <HelpIcon />;
    }
  };

  const calculateYAxisDomain = React.useCallback((): [number | string, number | string] => {
    if (indicator.yAxisDomain) return indicator.yAxisDomain as [number | string, number | string];
    
    let allValues: number[] = [];
    if (chartData.length > 0) {
        allValues.push(...chartData.map(d => d.value).filter(v => typeof v === 'number'));
    }
    if (indicator.ma_series_data && indicator.ma_series_data.length > 0) {
        allValues.push(...indicator.ma_series_data.map(d => d.value).filter(v => typeof v === 'number'));
    }

    if (allValues.length === 0) return ['auto', 'auto'];

    let dataMin = Math.min(...allValues);
    let dataMax = Math.max(...allValues);

    if (indicator.bullishThreshold !== undefined) {
        dataMin = Math.min(dataMin, indicator.bullishThreshold);
        dataMax = Math.max(dataMax, indicator.bullishThreshold);
    }
    if (indicator.bearishThreshold !== undefined) {
        dataMin = Math.min(dataMin, indicator.bearishThreshold);
        dataMax = Math.max(dataMax, indicator.bearishThreshold);
    }
     if (indicator.series_id.includes('ISM') || (indicator.bullishThreshold === 50 && (indicator.bearishThreshold === 45 || indicator.bearishThreshold === 50)) ) {
         dataMin = Math.min(dataMin, 30); 
         dataMax = Math.max(dataMax, 70);
     }

    if (dataMin === dataMax) {
        const padding = Math.abs(dataMin * 0.1) || 10; 
        return [dataMin - padding, dataMax + padding];
    }
    
    const range = dataMax - dataMin;
    const padding = range * 0.1; 

    return [dataMin - padding, dataMax + padding];
  }, [chartData, indicator.ma_series_data, indicator.yAxisDomain, indicator.bullishThreshold, indicator.bearishThreshold, indicator.series_id]);

  const yAxisDomain = calculateYAxisDomain();

  const needsReferenceLine50 = (): boolean => {
    return indicator.series_id.includes('ISM') ||
           (indicator.bullishThreshold === 50 && (indicator.bearishThreshold === 45 || indicator.bearishThreshold === 50));
  };

  const getLatestDate = (): string => {
    if (!chartData || chartData.length === 0) return 'N/A';
    const latestPoint = chartData[chartData.length -1];
    return formatDateDutch(latestPoint.originalDate);
  };

  const getMonthlyXAxisTicks = React.useCallback((): number[] => {
    if (!combinedChartData.length) return [];
    const monthlyTicksMap = new Map<string, number>();
    combinedChartData.forEach(point => {
      if (point && point.originalDate) { 
        const yearMonth = `${point.originalDate.getFullYear()}-${String(point.originalDate.getMonth() + 1).padStart(2, '0')}`;
        if (!monthlyTicksMap.has(yearMonth)) {
          monthlyTicksMap.set(yearMonth, point.timestamp);
        }
      }
    });
    return Array.from(monthlyTicksMap.values()).sort((a,b) => a - b);
  }, [combinedChartData]);

  const monthlyTicks = getMonthlyXAxisTicks();
  
  const maLabel = indicator.series_id === 'SP500' && indicator.bullishThreshold === indicator.bearishThreshold ? `MA (${(indicator.bullishThreshold || 125).toFixed(0)})` : 'MA';


  if (isLoading) {
    return (
      <Card sx={{ minHeight: 300, display: 'flex', justifyContent: 'center', alignItems: 'center', mb: 2 }}>
        <CircularProgress />
      </Card>
    );
  }

  if (error) {
    return (
      <Card sx={{ minHeight: 300, mb: 2 }}>
        <CardContent>
          <Typography color="error" variant="h6">Error</Typography>
          <Typography color="error">{error}</Typography>
        </CardContent>
      </Card>
    );
  }

  return (
    <Card sx={{ minHeight: 300, mb: 2, boxShadow: 3 }}>
      <CardContent sx={{ '&:last-child': { pb: 2 } }}>
        <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'flex-start', mb: 1 }}>
          <Box sx={{maxWidth: 'calc(100% - 100px)'}}>
            <Typography variant="h6" component="div" noWrap title={indicator.title}>
              {indicator.title}
            </Typography>
          </Box>
          <Chip
            icon={getSignalIcon(signalStatus)}
            label={signalStatus.toUpperCase()}
            sx={{ bgcolor: getSignalColor(signalStatus), color: 'white', fontWeight: 'bold', ml:1 }}
            size="small"
          />
        </Box>

        <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', mb: 1 }}>
          <Typography variant="body2" color="text.secondary">
            {indicator.frequency || ''} {indicator.units ? `(${indicator.units})` : ''}
            {chartData.length > 0 && (
              <span> • Last: {getLatestDate()}</span>
            )}
          </Typography>
          {lastValue !== undefined && (
            <Typography variant="h6" sx={{ fontWeight: 'medium' }}>
              {lastValue.toFixed(2)}
            </Typography>
          )}
        </Box>

        <Divider sx={{ mb: 2 }} />

        {combinedChartData.length > 0 ? (
          <Box sx={{ height: 220, width: '100%' }}>
            <ResponsiveContainer width="100%" height="100%">
              <LineChart
                data={combinedChartData} 
                margin={{ top: 5, right: 50, left: 0, bottom: 40 }} 
              >
                <CartesianGrid strokeDasharray="3 3" strokeOpacity={0.3} />
                <XAxis
                  dataKey="timestamp"
                  type="number"
                  scale="time"
                  domain={['dataMin', 'dataMax']}
                  ticks={monthlyTicks}
                  tickFormatter={(tickItem) => formatDateDutch(new Date(tickItem))}
                  angle={-45}
                  textAnchor="end"
                  height={55} 
                  dy={10}    
                  interval="preserveStartEnd" 
                  tick={{ fontSize: 10 }} 
                />
                <YAxis
                    domain={yAxisDomain}
                    allowDataOverflow={false}
                    tick={{ fontSize: 10 }}
                    width={45} 
                />
                <Tooltip
                  content={<CustomTooltipContent indicatorTitle={indicator.title} indicatorUnits={indicator.units} maLabel={maLabel}/>}
                  cursor={{ stroke: '#8884d8', strokeWidth: 1, strokeDasharray: '3 3' }}
                />
                {/* <Legend verticalAlign="top" height={30}/> Removed Legend */}

                {needsReferenceLine50() && (
                  <ReferenceLine y={50} stroke="#666" strokeDasharray="3 3" strokeOpacity={0.7} />
                )}

                {(!indicator.ma_series_data || indicator.bullishThreshold !== indicator.ma_series_data?.[indicator.ma_series_data.length-1]?.value) && indicator.bullishThreshold !== undefined && (
                  <ReferenceLine
                    y={indicator.bullishThreshold}
                    stroke={getSignalColor('bullish')}
                    strokeWidth={1}
                    strokeDasharray="5 5"
                    label={{
                      value: `Bull (${indicator.bullishThreshold.toFixed(1)})`,
                      position: 'insideTopRight',
                      dx: -5, dy: -2, fill: getSignalColor('bullish'), fontSize: 9, fontWeight: 'bold'
                    }}
                  />
                )}

                {(!indicator.ma_series_data || indicator.bearishThreshold !== indicator.ma_series_data?.[indicator.ma_series_data.length-1]?.value) && indicator.bearishThreshold !== undefined && (
                  <ReferenceLine
                    y={indicator.bearishThreshold}
                    stroke={getSignalColor('bearish')}
                    strokeWidth={1}
                    strokeDasharray="5 5"
                    label={{
                      value: `Bear (${indicator.bearishThreshold.toFixed(1)})`,
                      position: 'insideBottomRight',
                      dx: -5, dy: 2, fill: getSignalColor('bearish'), fontSize: 9, fontWeight: 'bold'
                    }}
                  />
                )}

                <Line
                  type="monotone"
                  dataKey="value"
                  name={indicator.title} 
                  stroke="#8884d8" 
                  strokeWidth={2}
                  dot={false}
                  activeDot={{ r: 5, strokeWidth: 0, fill: '#8884d8' }}
                />
                {indicator.ma_series_data && indicator.ma_series_data.length > 0 && (
                  <Line
                    type="monotone"
                    dataKey="maValue" 
                    name={maLabel} 
                    stroke="#ff7300" 
                    strokeWidth={2}
                    dot={false}
                    activeDot={{ r: 5, strokeWidth: 0, fill: '#ff7300' }}
                    connectNulls={true} 
                  />
                )}
              </LineChart>
            </ResponsiveContainer>
          </Box>
        ) : (
          <Box sx={{ height: 220, display: 'flex', justifyContent: 'center', alignItems: 'center' }}>
            <Typography variant="body2" color="text.secondary">
              No data available for the selected range.
            </Typography>
          </Box>
        )}

        {indicator.description && (
          <Typography variant="caption" color="text.secondary" sx={{ mt: 1.5, display: 'block', fontStyle: 'italic', lineHeight: 1.4, borderTop: '1px dashed', borderColor: 'divider', pt: 1 }}>
            {indicator.description}
          </Typography>
        )}
      </CardContent>
    </Card>
  );
};

export default IndicatorCard;


=== frontend\src\components\IndicatorCategoryPage.tsx ===

// frontend/src/components/IndicatorCategoryPage.tsx
import React, { useEffect, useState, useCallback, useMemo, useRef } from 'react';
import {
  Typography,
  Box,
  Button,
  Alert,
  Chip,
  CircularProgress,
  ButtonGroup,
} from '@mui/material';
import IndicatorService, {
  CategoryInfo,
  IndicatorsByTypeAPIResponse
} from '../services/IndicatorService';
import { IndicatorData } from './IndicatorCard';
// import { useIndicators } from '../context/IndicatorContext'; // setIndicator is commented out
import CategoryHeader from './CategoryHeader';
import CategoryNavigation, { Category as NavCategory } from './CategoryNavigation';
import IndicatorSection from './IndicatorSection';
import DashboardConfig from '../services/DashboardConfig';

// Helper function to convert API response to frontend IndicatorData format
const convertToIndicatorData = (apiResponse: EnrichedIndicatorAPIResponse): IndicatorData => {
  return {
    series_id: apiResponse.indicator_id,
    title: apiResponse.title,
    data: apiResponse.data.map(point => ({
      ...point,
      date: point.date,
    })),
    units: apiResponse.units,
    frequency: apiResponse.frequency,
    lastValue: apiResponse.last_value,
    bullishThreshold: apiResponse.bullish_threshold,
    bearishThreshold: apiResponse.bearish_threshold,
    signalStatus: apiResponse.signal_status,
    yAxisDomain: apiResponse.y_axis_domain,
    description: apiResponse.description,
  };
};

// Interface for the raw enriched indicator data from the API
interface EnrichedIndicatorAPIResponse {
  indicator_id: string;
  title: string;
  data: any[]; // Should be TimeSeriesPoint[]
  units?: string;
  frequency?: string;
  category: string;
  description?: string;
  bullish_threshold: number;
  bearish_threshold: number;
  signal_status: 'bullish' | 'bearish' | 'neutral';
  last_value?: number;
  last_updated?: string;
  y_axis_domain?: [number, number];
}

interface DateRangeConfigProp {
  startDate: string;
  endDate: string;
  label: string;
}

interface IndicatorCategoryPageProps {
  pageTitle: string;
  indicatorType: 'leading' | 'coincident' | 'lagging';
  dateRangeConfig: DateRangeConfigProp;
  apiInfoChipLabel?: string;
}

const IndicatorCategoryPage: React.FC<IndicatorCategoryPageProps> = ({
  pageTitle,
  indicatorType,
  dateRangeConfig: initialDateRangeConfig,
  apiInfoChipLabel
}) => {
  // const { setIndicator } = useIndicators(); // Commented out

  const [fetchedIndicatorDetails, setFetchedIndicatorDetails] = useState<Record<string, EnrichedIndicatorAPIResponse | null>>({});
  const [displayableCategories, setDisplayableCategories] = useState<CategoryInfo[]>([]);
  const [isPageLoading, setIsPageLoading] = useState<boolean>(true);
  const [pageError, setPageError] = useState<string | null>(null);
  const [initialLoadAttempted, setInitialLoadAttempted] = useState(false);

  const prevPropsRef = useRef<{indicatorType: string, activeDateRangeKey: keyof typeof DashboardConfig.dateRanges} | null>(null);

  const [activeDateRangeKey, setActiveDateRangeKey] = useState<keyof typeof DashboardConfig.dateRanges>(() => {
    const initialKey = Object.keys(DashboardConfig.dateRanges).find(
      key => DashboardConfig.dateRanges[key as keyof typeof DashboardConfig.dateRanges].label === initialDateRangeConfig.label
    ) as keyof typeof DashboardConfig.dateRanges | undefined;
    return initialKey || DashboardConfig.defaultDateRange;
  });

  const currentDateRange = useMemo(() => {
    return DashboardConfig.getDateRange(activeDateRangeKey);
  }, [activeDateRangeKey]);

  const fetchData = useCallback(async (isRefresh = false) => {
    // Set loading true for initial load or explicit refresh/timeframe change
    setIsPageLoading(true); 
    setPageError(null);

    const rangeToFetch = currentDateRange; // Uses the state-derived currentDateRange

    try {
      const response: IndicatorsByTypeAPIResponse = await IndicatorService.getIndicatorsByType(
        indicatorType,
        rangeToFetch.startDate,
        rangeToFetch.endDate
      );

      const newFetchedDetails: Record<string, EnrichedIndicatorAPIResponse | null> = {};
      response.indicators.forEach(indicator => {
        newFetchedDetails[indicator.indicator_id] = indicator;
        // setIndicator(indicator.indicator_id, convertToIndicatorData(indicator));
      });

      setFetchedIndicatorDetails(newFetchedDetails);
      setDisplayableCategories(response.categories || []);

      if (response.indicators.length === 0) {
        console.warn(`No indicators returned for type: ${indicatorType} for page: ${pageTitle}`);
        if (response.categories.length === 0) {
            setPageError(`No indicators or categories are currently configured for "${pageTitle}" (${indicatorType} type).`);
        }
      }
    } catch (error) {
      console.error(`Error fetching ${indicatorType} indicators for ${pageTitle}:`, error);
      setPageError(`An unexpected error occurred while loading indicators for ${pageTitle}. Please try refreshing.`);
      setFetchedIndicatorDetails({});
      setDisplayableCategories([]);
    } finally {
      setIsPageLoading(false);
      if (!initialLoadAttempted) {
        setInitialLoadAttempted(true);
      }
    }
  }, [indicatorType, pageTitle, currentDateRange, initialLoadAttempted /*, setIndicator, activeDateRangeKey is implicitly handled via currentDateRange */]);

  useEffect(() => {
    const previousIndicatorType = prevPropsRef.current?.indicatorType;
    const previousActiveDateRangeKey = prevPropsRef.current?.activeDateRangeKey;

    if (!initialLoadAttempted || indicatorType !== previousIndicatorType || activeDateRangeKey !== previousActiveDateRangeKey) {
        fetchData();
    }
    prevPropsRef.current = { indicatorType, activeDateRangeKey };

  }, [fetchData, initialLoadAttempted, indicatorType, activeDateRangeKey]);


  const handleRefreshAll = useCallback(async () => {
    await fetchData(true);
  }, [fetchData]);

  const handleTimeframeChange = (newTimeframeKey: keyof typeof DashboardConfig.dateRanges) => {
    setActiveDateRangeKey(newTimeframeKey);
    // fetchData will be called by useEffect due to activeDateRangeKey change
  };

  const mostRecentUpdateDate = useMemo(() => {
    const dates: Date[] = [];
    Object.values(fetchedIndicatorDetails).forEach(detail => {
      if (detail && detail.last_updated) {
        dates.push(new Date(detail.last_updated));
      } else if (detail && detail.data && detail.data.length > 0) {
        const lastPoint = detail.data[detail.data.length - 1];
        if (lastPoint && lastPoint.date) {
          dates.push(new Date(lastPoint.date));
        }
      }
    });

    if (dates.length === 0) return 'N/A';
    const mostRecentDate = new Date(Math.max(...dates.map(date => date.getTime())));
    return mostRecentDate.toLocaleDateString('en-US', { year: 'numeric', month: 'long', day: 'numeric' });
  }, [fetchedIndicatorDetails]);

  const navigationCategories = useMemo((): NavCategory[] => {
    return displayableCategories.map(cat => ({
      id: cat.category_id,
      name: cat.name
    }));
  }, [displayableCategories]);

  // Show full page loader only on the very first load attempt *before* initialLoadAttempted is true
  if (isPageLoading && !initialLoadAttempted) {
    return (
      <Box sx={{ display: 'flex', justifyContent: 'center', alignItems: 'center', minHeight: 300, flexDirection: 'column' }}>
        <CircularProgress sx={{ mb: 2 }} />
        <Typography>Loading {pageTitle}...</Typography>
      </Box>
    );
  }

  if (pageError && displayableCategories.length === 0 && Object.keys(fetchedIndicatorDetails).length === 0 && initialLoadAttempted) {
    return (
      <Box sx={{ p: 3 }}>
        <Typography variant="h5" component="h2" gutterBottom>{pageTitle}</Typography>
        <Alert severity="error" sx={{ mb: 3 }}>{pageError}</Alert>
        <Button variant="contained" onClick={() => fetchData(false)}>Try Again</Button>
      </Box>
    );
  }

  return (
    <Box>
      <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', mb: 3, flexWrap: 'wrap', gap: 2 }}>
        <Typography variant="h5" component="h2">
          {pageTitle}
        </Typography>
        <Box sx={{display: 'flex', gap: 1, flexWrap: 'wrap', alignItems: 'center'}}>
          <ButtonGroup variant="outlined" aria-label="timeframe selection button group">
            {(Object.keys(DashboardConfig.dateRanges) as Array<keyof typeof DashboardConfig.dateRanges>).map((key) => (
              <Button
                key={key}
                variant={activeDateRangeKey === key ? "contained" : "outlined"}
                onClick={() => handleTimeframeChange(key)}
              >
                {DashboardConfig.dateRanges[key].label.replace(" Years", "Y").replace(" Year", "Y")}
              </Button>
            ))}
          </ButtonGroup>
          <Button
            variant="contained"
            color="primary"
            onClick={handleRefreshAll}
            disabled={isPageLoading}
            sx={{ml: 1}}
          >
            {isPageLoading ? <CircularProgress size={24} color="inherit"/> : 'Refresh All Data'}
          </Button>
        </Box>
      </Box>

      {pageError && (initialLoadAttempted && (Object.keys(fetchedIndicatorDetails).length > 0 || displayableCategories.length > 0)) && (
         <Alert severity="warning" sx={{ mb: 3 }}>
           {pageError} Some data may be incomplete or outdated.
         </Alert>
      )}

      <Alert severity="info" sx={{ mb: 3 }}>
        <Box sx={{ display: 'flex', alignItems: 'center', gap: 1, flexWrap: 'wrap' }}>
          <Typography variant="body2">
            <strong>Data Range:</strong> {currentDateRange.startDate} to {currentDateRange.endDate}
          </Typography>
          <Chip
            label={`${currentDateRange.label}`}
            size="small"
            color="primary"
            variant="outlined"
          />
          {apiInfoChipLabel && (
            <Chip label={apiInfoChipLabel} size="small" color="success" variant="outlined"/>
          )}
        </Box>
      </Alert>

      {navigationCategories.length > 0 && <CategoryNavigation categories={navigationCategories} />}

      {displayableCategories.length === 0 && !isPageLoading && initialLoadAttempted && !pageError && (
         <Alert severity="info" sx={{ mt: 2 }}>
            No indicators are currently available for the "{pageTitle}" ({indicatorType}) type for the selected date range.
         </Alert>
      )}

      {displayableCategories.map((category) => (
        <Box key={category.category_id} id={category.category_id} sx={{ mb: 4, scrollMarginTop: '80px' }}>
          <CategoryHeader
            id={category.category_id}
            title={category.name}
            description={category.description}
          />
          
          {category.indicators.length === 0 ? (
            <Alert severity="info" sx={{mt: 2}}>No indicators currently listed in the "{category.name}" category for this type.</Alert>
          ) : (
            category.indicators.map((indicatorId: string) => {
              const indicatorDetail = fetchedIndicatorDetails[indicatorId];
              if (!indicatorDetail) {
                return (
                  <Box key={indicatorId} sx={{mb: 2}}> 
                    <Alert severity="warning">Data for indicator {indicatorId} is unavailable.</Alert>
                  </Box>
                );
              }
              const cardData = convertToIndicatorData(indicatorDetail);
              return (
                <IndicatorSection
                  key={indicatorId}
                  title={cardData.title}
                  data={cardData}
                  // THIS IS THE CORRECTED LINE:
                  isLoading={isPageLoading} 
                  error={null}    
                  defaultTitle={indicatorDetail.title || indicatorId}
                />
              );
            })
          )}
        </Box>
      ))}

      <Box sx={{ mt: 4, textAlign: 'right' }}>
        <Typography variant="caption" color="text.secondary">
          Last successful fetch: {(!isPageLoading && initialLoadAttempted) ? new Date().toLocaleString() : 'Loading...'}
        </Typography>
        <Typography variant="caption" color="text.secondary" display="block">
          Most recent data point: {mostRecentUpdateDate}
        </Typography>
         {apiInfoChipLabel && (
            <Typography variant="caption" color="text.secondary" display="block">
                Processing: Server-side ({apiInfoChipLabel}) • Signals: Auto-calculated • Transformations: Backend
            </Typography>
        )}
      </Box>
    </Box>
  );
};

export default IndicatorCategoryPage;


=== frontend\src\components\IndicatorSection.tsx ===

import React from 'react';
import { Box, Typography, Paper, CircularProgress } from '@mui/material';
import IndicatorCard, { IndicatorData } from './IndicatorCard';

interface IndicatorSectionProps {
  title: string;
  data: IndicatorData | null;
  isLoading: boolean;
  error: string | null | undefined;
  defaultTitle: string;
}

/**
 * A component that renders either an IndicatorCard with data or a loading/error placeholder
 */
const IndicatorSection: React.FC<IndicatorSectionProps> = ({
  title,
  data,
  isLoading,
  error,
  defaultTitle
}) => {
  return (
    <Box sx={{ mb: 3 }}>
      {data ? (
        <IndicatorCard 
          indicator={data}
          isLoading={isLoading}
          error={error}
        />
      ) : (
        <Paper sx={{ minHeight: 300, p: 2, display: 'flex', flexDirection: 'column', justifyContent: 'center', alignItems: 'center' }}>
          <Typography variant="h6" gutterBottom>{defaultTitle}</Typography>
          {isLoading ? (
            <>
              <Box sx={{ my: 2 }}>
                <CircularProgress />
              </Box>
              <Typography>Loading data...</Typography>
            </>
          ) : (
            <Typography color="text.secondary">No data available</Typography>
          )}
        </Paper>
      )}
    </Box>
  );
};

export default IndicatorSection;

=== frontend\src\components\LeadingIndicators.tsx ===

// // frontend/src/components/LeadingIndicators.tsx

// import React, { useEffect, useState, useCallback, useMemo } from 'react';
// import { 
//   Typography, 
//   Box, 
//   Button,
//   Alert,
//   Chip
// } from '@mui/material';
// import { IndicatorData } from './IndicatorCard';
// import IndicatorService from '../services/IndicatorService';
// import { useIndicators } from '../context/IndicatorContext';
// import CategoryHeader from './CategoryHeader';
// import CategoryNavigation from './CategoryNavigation';
// import IndicatorSection from './IndicatorSection';
// import DashboardConfig from '../services/DashboardConfig';

// const LeadingIndicators: React.FC = () => {
//   const { setIndicator } = useIndicators();
  
//   // Simplified state management - no more complex individual states
//   const [indicatorData, setIndicatorData] = useState<Record<string, IndicatorData>>({});
//   const [loadingStates, setLoadingStates] = useState<Record<string, boolean>>({});
//   const [errors, setErrors] = useState<Record<string, string>>({});
//   const [categories, setCategories] = useState<any[]>([]);
//   const [hasInitiallyLoaded, setHasInitiallyLoaded] = useState(false);
  
//   // Get date range from config
//   const dateRange = useMemo(() => DashboardConfig.getDefaultDateRange(), []);
  
//   // Leading indicator categories (filtered from backend)
//   const leadingCategories = useMemo(() => [
//     'Business Cycle Indicators',
//     'Global Risk Metrics', 
//     'Financial Market Indicators',
//     'Global Liquidity Metrics',
//     'Housing Market'
//   ], []);

//   const fetchCategoriesAndIndicators = useCallback(async () => {
//     try {
//       // Fetch categories from backend
//       const allCategories = await IndicatorService.getCategories();
      
//       // Filter for leading indicator categories
//       const leadingCats = allCategories.filter(cat => 
//         leadingCategories.includes(cat.name)
//       );
//       setCategories(leadingCats);
      
//       // Get all indicator IDs from leading categories
//       const allIndicatorIds = leadingCats.flatMap(cat => cat.indicators);
      
//       // Set initial loading states
//       const initialLoadingStates: Record<string, boolean> = {};
//       allIndicatorIds.forEach(id => {
//         initialLoadingStates[id] = true;
//       });
//       setLoadingStates(initialLoadingStates);
      
//       // Fetch all indicators in parallel (backend does all processing)
//       const indicatorDataResult = await IndicatorService.getMultipleIndicators(
//         allIndicatorIds,
//         dateRange.startDate,
//         dateRange.endDate
//       );
      
//       // Update state
//       setIndicatorData(indicatorDataResult);
      
//       // Update global context
//       Object.entries(indicatorDataResult).forEach(([id, data]) => {
//         setIndicator(id, data);
//       });
      
//       // Clear loading states
//       const clearedLoadingStates: Record<string, boolean> = {};
//       allIndicatorIds.forEach(id => {
//         clearedLoadingStates[id] = false;
//       });
//       setLoadingStates(clearedLoadingStates);
      
//       setHasInitiallyLoaded(true);
      
//     } catch (error) {
//       console.error('Error fetching categories and indicators:', error);
//       // Set error state for all indicators
//       const errorStates: Record<string, string> = {};
//       const allIndicatorIds = categories.flatMap(cat => cat.indicators);
//       allIndicatorIds.forEach(id => {
//         errorStates[id] = 'Failed to load data';
//       });
//       setErrors(errorStates);
//       setLoadingStates({});
//     }
//   }, [leadingCategories, categories, dateRange.startDate, dateRange.endDate, setIndicator]); // Remove loadingStates dependency

//   // Fetch categories and indicators on mount
//   useEffect(() => {
//     if (!hasInitiallyLoaded) {
//       fetchCategoriesAndIndicators();
//     }
//   }, [hasInitiallyLoaded, fetchCategoriesAndIndicators]);

//   const handleRefreshAll = useCallback(async () => {
//     // Reset states
//     setErrors({});
    
//     // Use current categories state directly (no dependency needed)
//     setCategories(currentCategories => {
//       const allIndicatorIds = currentCategories.flatMap(cat => cat.indicators);
      
//       // Set all to loading
//       const newLoadingStates: Record<string, boolean> = {};
//       allIndicatorIds.forEach(id => {
//         newLoadingStates[id] = true;
//       });
//       setLoadingStates(newLoadingStates);
      
//       // Fetch fresh data async
//       (async () => {
//         try {
//           const indicatorDataResult = await IndicatorService.getMultipleIndicators(
//             allIndicatorIds,
//             dateRange.startDate,
//             dateRange.endDate
//           );
          
//           setIndicatorData(indicatorDataResult);
          
//           // Update global context
//           Object.entries(indicatorDataResult).forEach(([id, data]) => {
//             setIndicator(id, data);
//           });
          
//         } catch (error) {
//           console.error('Error refreshing indicators:', error);
//           const errorStates: Record<string, string> = {};
//           allIndicatorIds.forEach(id => {
//             errorStates[id] = 'Failed to refresh data';
//           });
//           setErrors(errorStates);
//         } finally {
//           // Clear loading states
//           setLoadingStates({});
//         }
//       })();
      
//       return currentCategories; // Return unchanged categories
//     });
//   }, [dateRange.startDate, dateRange.endDate, setIndicator]);

//   // Get the most recent update date
//   const mostRecentUpdateDate = useMemo(() => {
//     const dates: Date[] = [];
    
//     Object.values(indicatorData).forEach(data => {
//       if (data.data && data.data.length > 0) {
//         dates.push(new Date(data.data[data.data.length - 1].date));
//       }
//     });
    
//     if (dates.length === 0) {
//       return 'No data available';
//     }
    
//     const mostRecentDate = new Date(Math.max(...dates.map(date => date.getTime())));
//     return mostRecentDate.toLocaleDateString('en-US', {
//       year: 'numeric',
//       month: 'long',
//       day: 'numeric'
//     });
//   }, [indicatorData]);

//   // Create navigation categories for UI
//   const navigationCategories = useMemo(() => {
//     return categories.map(cat => ({
//       id: cat.category_id,
//       name: cat.name
//     }));
//   }, [categories]);

//   // Show loading message if still fetching categories
//   if (!hasInitiallyLoaded && categories.length === 0) {
//     return (
//       <Box sx={{ display: 'flex', justifyContent: 'center', alignItems: 'center', minHeight: 200 }}>
//         <Typography>Loading categories and indicators...</Typography>
//       </Box>
//     );
//   }

//   return (
//     <Box>
//       <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', mb: 3 }}>
//         <Typography variant="h5" component="h2">
//           Leading Indicators
//         </Typography>
//         <Button 
//           variant="contained" 
//           color="primary" 
//           onClick={handleRefreshAll}
//           disabled={Object.values(loadingStates).some(loading => loading)}
//         >
//           Refresh All Data
//         </Button>
//       </Box>
      
//       {/* Show data range info */}
//       <Alert severity="info" sx={{ mb: 3 }}>
//         <Box sx={{ display: 'flex', alignItems: 'center', gap: 1, flexWrap: 'wrap' }}>
//           <Typography variant="body2">
//             <strong>Data Range:</strong> {dateRange.startDate} to {dateRange.endDate}
//           </Typography>
//           <Chip 
//             label={`${dateRange.label}`} 
//             size="small" 
//             color="primary" 
//             variant="outlined" 
//           />
//           <Chip 
//             label="v2 API (Backend Processing)" 
//             size="small" 
//             color="success" 
//             variant="outlined" 
//           />
//         </Box>
//       </Alert>
      
//       {/* Category Navigation */}
//       <CategoryNavigation categories={navigationCategories} />
      
//       {/* Render each category */}
//       {categories.map((category) => (
//         <Box key={category.category_id} id={category.category_id} sx={{ mb: 4, scrollMarginTop: '80px' }}>
//           <CategoryHeader 
//             id={category.category_id}
//             title={category.name}
//             description={category.description}
//           />
          
//           {/* Render indicators in this category */}
//           {category.indicators.map((indicatorId: string) => (
//             <IndicatorSection
//               key={indicatorId}
//               title={indicatorId}
//               data={indicatorData[indicatorId] || null}
//               isLoading={loadingStates[indicatorId] || false}
//               error={errors[indicatorId] || null}
//               defaultTitle={indicatorId}
//             />
//           ))}
//         </Box>
//       ))}
      
//       <Box sx={{ mt: 4, textAlign: 'right' }}>
//         <Typography variant="caption" color="text.secondary">
//           Last fetched: {hasInitiallyLoaded ? new Date().toLocaleString() : 'Loading...'}
//         </Typography>
//         <Typography variant="caption" color="text.secondary" display="block">
//           Most recent data point: {mostRecentUpdateDate}
//         </Typography>
//         <Typography variant="caption" color="text.secondary" display="block">
//           Processing: Server-side (v2 API) • Signals: Auto-calculated • Transformations: Backend
//         </Typography>
//       </Box>
//     </Box>
//   );
// };

// export default LeadingIndicators;
export {}

=== frontend\src\components\index.ts ===

// Export all components from a single file for easier imports
export { default as CategoryHeader } from './CategoryHeader';
export { default as CategoryNavigation } from './CategoryNavigation';
export { default as IndicatorCard } from './IndicatorCard';
export { default as IndicatorSection } from './IndicatorSection';

// Also export types
export type { IndicatorData, TimeSeriesPoint, SignalStatus } from './IndicatorCard';

=== frontend\src\context\IndicatorContext.tsx ===

// frontend/src/context/IndicatorContext.tsx

import React, { createContext, useState, useContext, ReactNode, useCallback, useMemo } from 'react'; // Import useMemo
import { IndicatorData } from '../components/IndicatorCard';

interface IndicatorContextType {
  indicators: Record<string, IndicatorData | null>;
  setIndicator: (key: string, data: IndicatorData | null) => void;
  calculateMarketStatus: () => { bullBear: 'BULL' | 'BEAR' | 'NEUTRAL', riskOnOff: 'RISK-ON' | 'RISK-OFF' | 'NEUTRAL' };
}

const IndicatorContext = createContext<IndicatorContextType | undefined>(undefined);

export const IndicatorProvider: React.FC<{ children: ReactNode }> = ({ children }) => {
  const [indicators, setIndicators] = useState<Record<string, IndicatorData | null>>({});

  // This is already stable - which is good!
  const setIndicator = useCallback((key: string, data: IndicatorData | null) => {
    setIndicators(prev => ({
      ...prev,
      [key]: data
    }));
  }, []);

  const calculateMarketStatus = useCallback(() => {
    let bullishCount = 0;
    let bearishCount = 0;
    let neutralCount = 0;
    let riskOnCount = 0;
    let riskOffCount = 0;

    Object.entries(indicators).forEach(([key, indicator]) => {
      if (!indicator) return;
      const lastValue = indicator.lastValue !== undefined ? indicator.lastValue : (indicator.data?.length > 0 ? indicator.data[indicator.data.length - 1].value : undefined);

      // Ensure thresholds are numbers or explicitly handle if they can be undefined
      const bullishThreshold = typeof indicator.bullishThreshold === 'number' ? indicator.bullishThreshold : undefined;
      const bearishThreshold = typeof indicator.bearishThreshold === 'number' ? indicator.bearishThreshold : undefined;

      if (lastValue === undefined || bullishThreshold === undefined || bearishThreshold === undefined) {
        neutralCount++;
        return;
      }

      if (lastValue >= bullishThreshold) {
        bullishCount++;
      } else if (lastValue <= bearishThreshold) {
        bearishCount++;
      } else {
        neutralCount++;
      }

      // Simplified risk logic based on your existing structure, ensure thresholds are valid
      if (key.includes('ISM')) {
        if (lastValue >= 50) riskOnCount++; else riskOffCount++;
      } else if (key === 'GOLD-COPPER-RATIO') {
        if (bullishThreshold !== undefined && lastValue <= bullishThreshold) riskOnCount++;
        else if (bearishThreshold !== undefined && lastValue >= bearishThreshold) riskOffCount++;
      } else if (key === 'FRED-T10Y2Y') {
        if (lastValue > 0) riskOnCount++; else riskOffCount++;
      } else if (key === 'FRED-VIXCLS') {
        if (bullishThreshold !== undefined && lastValue >= bullishThreshold) riskOnCount++;
        else if (bearishThreshold !== undefined && lastValue <= bearishThreshold) riskOffCount++;
      } else if (key === 'FRED-M2SL') {
        if (bullishThreshold !== undefined && lastValue >= bullishThreshold) riskOnCount++;
        else if (bearishThreshold !== undefined && lastValue <= bearishThreshold) riskOffCount++;
      } else if (key.startsWith('FRED-')) {
        if (bullishThreshold !== undefined && lastValue >= bullishThreshold) riskOnCount++;
        else if (bearishThreshold !== undefined && lastValue <= bearishThreshold) riskOffCount++;
      }
    });

    let bullBear: 'BULL' | 'BEAR' | 'NEUTRAL';
    let riskOnOff: 'RISK-ON' | 'RISK-OFF' | 'NEUTRAL';
    const totalValuedIndicators = bullishCount + bearishCount + neutralCount;

    if (totalValuedIndicators === 0) {
      bullBear = 'NEUTRAL';
      riskOnOff = 'NEUTRAL';
    } else {
      const bullishScore = bullishCount / totalValuedIndicators;
      const bearishScore = bearishCount / totalValuedIndicators;

      if (bullishScore > 0.6) bullBear = 'BULL';
      else if (bearishScore > 0.6) bullBear = 'BEAR';
      else if (bullishCount > bearishCount) bullBear = 'BULL';
      else if (bearishCount > bullishCount) bullBear = 'BEAR';
      else bullBear = 'NEUTRAL';

      if (riskOnCount > riskOffCount) riskOnOff = 'RISK-ON';
      else if (riskOffCount > riskOnCount) riskOnOff = 'RISK-OFF';
      else riskOnOff = 'NEUTRAL';
    }
    return { bullBear, riskOnOff };
  }, [indicators]);

  // FIX: Memoize the context value object
  const contextValue = useMemo(() => ({
    indicators,
    setIndicator,         // Stable reference from its own useCallback
    calculateMarketStatus // Reference changes when `indicators` state changes
  }), [indicators, setIndicator, calculateMarketStatus]);
  // `setIndicator` is stable. `calculateMarketStatus` changes when `indicators` changes.
  // So, `contextValue` will get a new reference primarily when `indicators` (data) changes.

  return (
    <IndicatorContext.Provider value={contextValue}>
      {children}
    </IndicatorContext.Provider>
  );
};

export const useIndicators = () => {
  const context = useContext(IndicatorContext);
  if (context === undefined) {
    throw new Error('useIndicators must be used within an IndicatorProvider');
  }
  return context;
};

=== frontend\src\index.css ===

body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}


=== frontend\src\index.tsx ===

import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';
import reportWebVitals from './reportWebVitals';

const root = ReactDOM.createRoot(
  document.getElementById('root') as HTMLElement
);
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);

// If you want to start measuring performance in your app, pass a function
// to log results (for example: reportWebVitals(console.log))
// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
reportWebVitals();


=== frontend\src\pages\Dashboard.tsx ===

// frontend/src/pages/Dashboard.tsx
import React, { useState, useCallback, useMemo, useEffect } from 'react';
import {
  Box,
  Container,
  Typography,
  Paper,
  Tabs,
  Tab,
  Card,
  CardContent,
  Grid,
  LinearProgress,
  Chip,
  Alert
} from '@mui/material';
import {
  TrendingUp as BullIcon,
  TrendingDown as BearIcon,
  Help as NeutralIcon,
  Speed as RiskOnIcon,
  Security as RiskOffIcon
} from '@mui/icons-material';
import IndicatorCategoryPage from '../components/IndicatorCategoryPage';
import IndicatorService from '../services/IndicatorService';
import DashboardConfig from '../services/DashboardConfig';

interface MarketStatus {
  bull_bear_status: string;
  risk_on_off_status: string;
  bull_bear_score: number;
  risk_on_off_score: number;
  total_indicators: number;
  bullish_count: number;
  bearish_count: number;
  neutral_count: number;
  last_updated: string;
}

// These constants are no longer needed as IndicatorCategoryPage fetches by type
// const LEADING_INDICATOR_CATEGORIES_NAMES = [
//   'Business Cycle Indicators',
//   'Global Risk Metrics',
//   'Financial Market Indicators',
//   'Global Liquidity Metrics',
//   'Housing Market'
// ];

// const COINCIDENT_INDICATOR_CATEGORIES_NAMES = [
//   'Market Sentiment',
//   'Economic Activity'
// ];

// const LAGGING_INDICATOR_CATEGORIES_NAMES: string[] = [
//     // Example: 'Inflation Metrics', 'Employment Trends'
// ];


function Dashboard() {
  const [tabValue, setTabValue] = useState(0);
  const [marketStatus, setMarketStatus] = useState<MarketStatus | null>(null);
  const [marketStatusLoading, setMarketStatusLoading] = useState(true);
  const [marketStatusError, setMarketStatusError] = useState<string | null>(null);

  const handleTabChange = useCallback((event: React.SyntheticEvent, newValue: number) => {
    setTabValue(newValue);
  }, []);

  const fetchMarketStatus = useCallback(async () => {
    try {
      setMarketStatusLoading(true);
      setMarketStatusError(null);
      const status = await IndicatorService.getMarketStatus();
      setMarketStatus(status);
    } catch (error) {
      console.error('Error fetching market status:', error);
      setMarketStatusError('Failed to load market status from the server.');
      setMarketStatus(null);
    } finally {
      setMarketStatusLoading(false);
    }
  }, []);

  useEffect(() => {
    fetchMarketStatus();
    const interval = setInterval(fetchMarketStatus, 5 * 60 * 1000); // Refresh every 5 minutes
    return () => clearInterval(interval);
  }, [fetchMarketStatus]);

  const getBullBearDisplay = useMemo(() => {
    if (marketStatusLoading) return { color: 'text.secondary', icon: <NeutralIcon />, label: 'LOADING', bgColor: 'grey.700' };
    if (!marketStatus || marketStatusError) return { color: 'text.secondary', icon: <NeutralIcon />, label: 'N/A', bgColor: 'grey.700' };

    switch (marketStatus.bull_bear_status) {
      case 'BULL':
        return { color: 'success.main', bgColor: 'success.dark', icon: <BullIcon />, label: 'BULL MARKET' };
      case 'BEAR':
        return { color: 'error.main', bgColor: 'error.dark', icon: <BearIcon />, label: 'BEAR MARKET' };
      default:
        return { color: 'warning.main', bgColor: 'warning.dark', icon: <NeutralIcon />, label: 'NEUTRAL MARKET' };
    }
  }, [marketStatus, marketStatusLoading, marketStatusError]);

  const getRiskDisplay = useMemo(() => {
    if (marketStatusLoading) return { color: 'text.secondary', icon: <NeutralIcon />, label: 'LOADING', bgColor: 'grey.700' };
    if (!marketStatus || marketStatusError) return { color: 'text.secondary', icon: <NeutralIcon />, label: 'N/A', bgColor: 'grey.700' };

    switch (marketStatus.risk_on_off_status) {
      case 'RISK-ON':
        return { color: 'info.main', bgColor: 'info.dark', icon: <RiskOnIcon />, label: 'RISK-ON' };
      case 'RISK-OFF':
        return { color: 'warning.main', bgColor: 'warning.dark', icon: <RiskOffIcon />, label: 'RISK-OFF' };
      default:
        return { color: 'grey.600', bgColor: 'grey.700', icon: <NeutralIcon />, label: 'NEUTRAL' };
    }
  }, [marketStatus, marketStatusLoading, marketStatusError]);

  //const defaultDateRange = DashboardConfig.getDefaultDateRange();
  // Using a consistent date range for all types for now, can be customized per tab if needed.
  // For example, leading indicators might use a longer historical range.
  const leadingDateRange = DashboardConfig.getDateRange('4Y'); // Example: 5 years for leading
  const coincidentDateRange = DashboardConfig.getDateRange('4Y'); // Example: 3 years for coincident
  const laggingDateRange = DashboardConfig.getDateRange('4Y'); // Example: 5 years for lagging


  const apiInfoChip = "v2 API (Backend Processing)";

  return (
    <Container maxWidth="xl">
      <Box sx={{ my: 4 }}>
        <Typography variant="h4" component="h1" gutterBottom align="center">
          Macro Investment Dashboard
        </Typography>

        <Grid container spacing={3} sx={{ mb: 3 }}>
          <Grid item xs={12} md={6}>
            <Card sx={{ bgcolor: getBullBearDisplay.bgColor, color: 'white', textAlign: 'center', minHeight: 140, display: 'flex', flexDirection: 'column', justifyContent: 'center' }}>
              <CardContent>
                <Box sx={{ display: 'flex', alignItems: 'center', justifyContent: 'center', mb: 1 }}>
                  {getBullBearDisplay.icon}
                  <Typography variant="h6" component="div" sx={{ ml: 1 }}>Market Cycle</Typography>
                </Box>
                {marketStatusLoading ? (
                  <Box sx={{ mt: 2 }}><LinearProgress color="inherit" /><Typography variant="body2" sx={{ mt: 1 }}>Loading...</Typography></Box>
                ) : marketStatusError && !marketStatus ? (
                  <Box sx={{ mt: 1 }}><Typography variant="h4" component="div" sx={{ mb: 0.5 }}>{getBullBearDisplay.label}</Typography><Typography variant="caption" sx={{ color: 'rgba(255,255,255,0.7)' }}>{marketStatusError}</Typography></Box>
                ) : marketStatus ? (
                  <><Typography variant="h4" component="div" sx={{ mt: 1, mb: 0.5 }}>{getBullBearDisplay.label}</Typography><Box sx={{ mt: 1 }}><LinearProgress variant="determinate" value={marketStatus.bull_bear_score || 0} color="inherit" sx={{ height: 8, borderRadius: 4 }} /><Typography variant="caption" sx={{ mt: 0.5, display: 'block' }}>Confidence: {marketStatus.bull_bear_score.toFixed(1)}%</Typography></Box></>
                ) : (
                  <Typography sx={{mt: 1}}>Market status unavailable.</Typography>
                )}
              </CardContent>
            </Card>
          </Grid>
          <Grid item xs={12} md={6}>
            <Card sx={{ bgcolor: getRiskDisplay.bgColor, color: 'white', textAlign: 'center', minHeight: 140, display: 'flex', flexDirection: 'column', justifyContent: 'center' }}>
              <CardContent>
                <Box sx={{ display: 'flex', alignItems: 'center', justifyContent: 'center', mb: 1 }}>
                  {getRiskDisplay.icon}
                  <Typography variant="h6" component="div" sx={{ ml: 1 }}>Risk Sentiment</Typography>
                </Box>
                 {marketStatusLoading ? (
                  <Box sx={{ mt: 2 }}><LinearProgress color="inherit" /><Typography variant="body2" sx={{ mt: 1 }}>Loading...</Typography></Box>
                ) : marketStatusError && !marketStatus ? (
                  <Box sx={{ mt: 1 }}><Typography variant="h4" component="div" sx={{ mb: 0.5 }}>{getRiskDisplay.label}</Typography><Typography variant="caption" sx={{ color: 'rgba(255,255,255,0.7)' }}>{marketStatusError}</Typography></Box>
                ) : marketStatus ? (
                  <><Typography variant="h4" component="div" sx={{ mt: 1, mb: 0.5 }}>{getRiskDisplay.label}</Typography><Box sx={{ mt: 1 }}><LinearProgress variant="determinate" value={marketStatus.risk_on_off_score || 0} color="inherit" sx={{ height: 8, borderRadius: 4 }} /><Typography variant="caption" sx={{ mt: 0.5, display: 'block' }}>Confidence: {marketStatus.risk_on_off_score.toFixed(1)}%</Typography></Box></>
                ) : (
                  <Typography sx={{mt: 1}}>Risk sentiment unavailable.</Typography>
                )}
              </CardContent>
            </Card>
          </Grid>
        </Grid>

        {marketStatusError && !marketStatusLoading && ( // Show general error for market status if it failed
           <Alert severity="warning" sx={{ mb: 3}}>{marketStatusError}</Alert>
        )}
        {marketStatus && !marketStatusLoading && !marketStatusError && (
          <Alert severity="info" sx={{ mb: 3 }}>
            <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', flexWrap: 'wrap', gap: 1 }}>
              <Typography variant="body2">
                <strong>Signal Breakdown:</strong> {marketStatus.bullish_count} Bullish, {marketStatus.bearish_count} Bearish, {marketStatus.neutral_count} Neutral
                {marketStatus.total_indicators > 0 && ` (${marketStatus.total_indicators} total indicators)`}
              </Typography>
              <Box sx={{ display: 'flex', gap: 1, alignItems: 'center', flexWrap: 'wrap' }}>
                <Chip label={`Updated: ${new Date(marketStatus.last_updated).toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })}`} size="small" variant="outlined"/>
                <Chip label="Auto-refresh (5min)" size="small" color="primary" variant="outlined" />
              </Box>
            </Box>
          </Alert>
        )}

        <Paper sx={{ width: '100%', mb: 3, position: 'sticky', top: 0, zIndex: 1100, bgcolor: 'background.paper' }}>
          <Tabs value={tabValue} onChange={handleTabChange} indicatorColor="primary" textColor="primary" centered sx={{ '& .MuiTab-root': { fontSize: '1rem', fontWeight: 'medium', py: 2 }}}>
            <Tab label="Leading Indicators" />
            <Tab label="Coincident Indicators" />
            <Tab label="Lagging Indicators" />
          </Tabs>
        </Paper>

        {/* Tab 0: Leading Indicators */}
        {tabValue === 0 && (
          <IndicatorCategoryPage
            pageTitle="Leading Indicators"
            indicatorType="leading" // Pass indicator type
            dateRangeConfig={{
              startDate: leadingDateRange.startDate,
              endDate: leadingDateRange.endDate,
              label: leadingDateRange.label
            }}
            apiInfoChipLabel={apiInfoChip}
          />
        )}

        {/* Tab 1: Coincident Indicators */}
        {tabValue === 1 && (
          <IndicatorCategoryPage
            pageTitle="Coincident Indicators"
            indicatorType="coincident" // Pass indicator type
            dateRangeConfig={{
              startDate: coincidentDateRange.startDate,
              endDate: coincidentDateRange.endDate,
              label: coincidentDateRange.label
            }}
            apiInfoChipLabel={apiInfoChip}
          />
        )}

        {/* Tab 2: Lagging Indicators */}
        {tabValue === 2 && (
           <IndicatorCategoryPage
                pageTitle="Lagging Indicators"
                indicatorType="lagging" // Pass indicator type
                dateRangeConfig={{
                    startDate: laggingDateRange.startDate,
                    endDate: laggingDateRange.endDate,
                    label: laggingDateRange.label
                }}
                apiInfoChipLabel={apiInfoChip}
            />
           // Placeholder if no lagging indicators are configured yet by the backend for this type
           // The IndicatorCategoryPage itself will show a message if no indicators are returned.
        )}
      </Box>

      <Box sx={{ mb: 4, mt: 8, p: 2, bgcolor: 'background.paper', borderRadius: 1, textAlign: 'center' }}>
        <Typography variant="subtitle2" color="text.secondary">
          <strong>Data Sources:</strong> Federal Reserve Economic Data (FRED), ISM, DBNomics, Yahoo Finance
        </Typography>
        <Typography variant="caption" color="text.secondary" display="block" sx={{ mt: 0.5 }}>
          <strong>Backend Processing:</strong> All transformations, signals, and calculations performed server-side (v2 API)
        </Typography>
        <Typography variant="caption" color="text.secondary" display="block">
          <strong>Dashboard loaded:</strong> {new Date().toLocaleString()}
        </Typography>
        {marketStatus && !marketStatusLoading && (
          <Typography variant="caption" color="text.secondary" display="block">
            <strong>Market status calculated:</strong> {new Date(marketStatus.last_updated).toLocaleString()}
          </Typography>
        )}
      </Box>
    </Container>
  );
}

export default Dashboard;


=== frontend\src\react-app-env.d.ts ===

/// <reference types="react-scripts" />


=== frontend\src\reportWebVitals.ts ===

import { ReportHandler } from 'web-vitals';

const reportWebVitals = (onPerfEntry?: ReportHandler) => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;


=== frontend\src\services\DashboardConfig.ts ===

// frontend/src/services/DashboardConfig.ts

export interface DashboardCategory {
  id: string;
  name: string;
  description: string;
  displayOrder: number;
}

export interface IndicatorDisplayConfig {
  id: string;
  displayName?: string;
  highlighted?: boolean;
  chartHeight?: number;
}

// UI-focused category definitions (order matters for display)
export const DASHBOARD_CATEGORIES: DashboardCategory[] = [
  {
    id: 'business-cycle',
    name: 'Business Cycle Indicators',
    description: 'Track economic expansion and contraction cycles',
    displayOrder: 1
  },
  {
    id: 'global-risk',
    name: 'Global Risk Metrics',
    description: 'Monitor investor sentiment and risk appetite',
    displayOrder: 2
  },
  {
    id: 'financial-markets',
    name: 'Financial Market Indicators',
    description: 'Track equity and credit market trends',
    displayOrder: 3
  },
  {
    id: 'liquidity',
    name: 'Global Liquidity Metrics',
    description: 'Monitor money supply and liquidity conditions',
    displayOrder: 4
  },
  {
    id: 'housing',
    name: 'Housing Market',
    description: 'Leading indicators from the housing sector',
    displayOrder: 5
  },
  {
    id: 'economic-activity',
    name: 'Economic Activity',
    description: 'Real-time economic performance metrics',
    displayOrder: 6
  },
  {
    id: 'market-sentiment',
    name: 'Market Sentiment',
    description: 'Current investor mood and market participation',
    displayOrder: 7
  }
];

// Featured indicators to highlight on the dashboard
export const FEATURED_INDICATORS: string[] = [
  'ISM-PMI',
  'VIX',
  'GOLD-COPPER-RATIO',
  'T10Y2Y',
  'SP500'
];

// Default date ranges for different views
export const DATE_RANGES = {
  '1Y': { months: 12, label: '1 Year' },
  '2Y': { months: 24, label: '2 Years' },
  '3Y': { months: 36, label: '3 Years' },
  '4Y': { months: 48, label: '4 Years' },
  '5Y': { months: 60, label: '5 Years' },
  '10Y': { months: 120, label: '10 Years' },
  '20Y': { months: 240, label: '20 Years' }
} as const; // 'as const' makes keys and properties readonly and more specific

// Explicitly type DEFAULT_DATE_RANGE as one of the keys of DATE_RANGES
export const DEFAULT_DATE_RANGE: keyof typeof DATE_RANGES = '4Y';

// Utility functions for UI
export const getCategoryById = (categoryId: string): DashboardCategory | undefined => {
  return DASHBOARD_CATEGORIES.find(cat => cat.id === categoryId);
};

export const getCategoriesSorted = (): DashboardCategory[] => {
  return [...DASHBOARD_CATEGORIES].sort((a, b) => a.displayOrder - b.displayOrder);
};

export const getDateRange = (rangeKey: keyof typeof DATE_RANGES) => {
  const range = DATE_RANGES[rangeKey];
  const endDate = new Date();
  const startDate = new Date();
  startDate.setMonth(endDate.getMonth() - range.months);

  return {
    startDate: startDate.toISOString().split('T')[0],
    endDate: endDate.toISOString().split('T')[0],
    label: range.label
  };
};

export const getDefaultDateRangeInfo = () => { // Renamed to avoid confusion with the key constant
  return getDateRange(DEFAULT_DATE_RANGE);
};

const DashboardConfig = {
  categories: DASHBOARD_CATEGORIES,
  featuredIndicators: FEATURED_INDICATORS,
  dateRanges: DATE_RANGES, // Correctly 'dateRanges' (lowercase 'd')
  defaultDateRange: DEFAULT_DATE_RANGE, // This is now correctly typed
  getCategoryById,
  getCategoriesSorted,
  getDateRange,
  getDefaultDateRangeInfo // Use the new name for the info object
};

export default DashboardConfig;


=== frontend\src\services\IndicatorService.ts ===

// frontend/src/services/IndicatorService.ts
import axios from 'axios';
import { IndicatorData, TimeSeriesPoint } from '../components/IndicatorCard'; // Assuming IndicatorCard exports these

const API_BASE_URL = process.env.REACT_APP_API_BASE_URL || 'http://localhost:8000/api';

const apiClient = axios.create({
  baseURL: API_BASE_URL,
  headers: {
    'Content-Type': 'application/json',
  },
});

// Interface for the raw response from /v2/indicators/{indicator_id}
export interface EnrichedIndicatorAPIResponse { // Exporting for IndicatorCategoryPage
  indicator_id: string;
  title: string;
  data: TimeSeriesPoint[];
  units?: string;
  frequency?: string;
  category: string;
  description?: string;
  bullish_threshold: number;
  bearish_threshold: number;
  signal_status: 'bullish' | 'bearish' | 'neutral';
  last_value?: number;
  last_updated?: string; 
  y_axis_domain?: [number, number];
  ma_series_data?: TimeSeriesPoint[]; // Added MA series data
}

// Interface for the metadata response from /v2/indicators/{indicator_id}/metadata or /v2/indicators/
interface IndicatorMetadataResponse {
  indicator_id: string;
  name: string;
  category: string;
  data_source: string;
  description?: string;
  units?: string;
  frequency?: string;
}

// Interface for category information from /v2/indicators/categories
export interface CategoryInfo { 
  category_id: string; 
  name: string; 
  description: string;
  indicators: string[]; 
}

// Interface for the market status response from /v2/indicators/market-status
interface MarketStatusResponse {
  bull_bear_status: string; 
  risk_on_off_status: string; 
  bull_bear_score: number;
  risk_on_off_score: number;
  total_indicators: number;
  bullish_count: number;
  bearish_count: number;
  neutral_count: number;
  last_updated: string; 
}

// Interface for the response from /v2/indicators/type/{indicator_type_value}
export interface IndicatorsByTypeAPIResponse { 
  indicator_type: string; 
  indicators: EnrichedIndicatorAPIResponse[];
  categories: CategoryInfo[]; 
}

// Helper to convert API response to frontend IndicatorData format
// This function is now also defined in IndicatorCategoryPage.tsx.
// For consistency, it's good to have it in one place, e.g., here, and export it.
// However, to avoid breaking the existing structure if IndicatorCategoryPage relies on its local version,
// I will ensure this one is up-to-date.
export const convertAPIToIndicatorData = (apiResponse: EnrichedIndicatorAPIResponse): IndicatorData => {
  return {
    series_id: apiResponse.indicator_id,
    title: apiResponse.title,
    data: apiResponse.data.map(point => ({
      ...point,
      date: point.date, 
    })),
    units: apiResponse.units,
    frequency: apiResponse.frequency,
    lastValue: apiResponse.last_value,
    bullishThreshold: apiResponse.bullish_threshold,
    bearishThreshold: apiResponse.bearish_threshold,
    signalStatus: apiResponse.signal_status,
    yAxisDomain: apiResponse.y_axis_domain,
    description: apiResponse.description, // Pass description
    ma_series_data: apiResponse.ma_series_data, // Pass MA series data
  };
};


const IndicatorService = {
  async getIndicator(indicatorId: string, startDate?: string, endDate?: string): Promise<IndicatorData> {
    try {
      const timestamp = new Date().getTime(); 
      const response = await apiClient.get<EnrichedIndicatorAPIResponse>(`/v2/indicators/${indicatorId}`, {
        params: { start_date: startDate, end_date: endDate, _t: timestamp },
      });
      return convertAPIToIndicatorData(response.data);
    } catch (error) {
      console.error(`Error fetching indicator ${indicatorId}:`, error);
      throw error;
    }
  },

  async getAllIndicatorsMetadata(): Promise<IndicatorMetadataResponse[]> {
    try {
      const response = await apiClient.get<IndicatorMetadataResponse[]>('/v2/indicators/'); 
      return response.data;
    } catch (error) {
      console.error('Error fetching all indicators metadata:', error);
      throw error;
    }
  },

  async getIndicatorMetadata(indicatorId: string): Promise<IndicatorMetadataResponse> {
    try {
      const response = await apiClient.get<IndicatorMetadataResponse>(`/v2/indicators/${indicatorId}/metadata`);
      return response.data;
    } catch (error) {
      console.error(`Error fetching metadata for ${indicatorId}:`, error);
      throw error;
    }
  },

  async getCategories(): Promise<CategoryInfo[]> {
    try {
      const response = await apiClient.get<CategoryInfo[]>('/v2/indicators/categories');
      return response.data;
    } catch (error) {
      console.error('Error fetching categories:', error);
      throw error;
    }
  },

  async getMarketStatus(indicatorIds?: string[]): Promise<MarketStatusResponse> {
    try {
      const params: any = {};
      if (indicatorIds && indicatorIds.length > 0) {
        params.indicators = indicatorIds.join(',');
      }
      const response = await apiClient.get<MarketStatusResponse>('/v2/indicators/market-status', { params });
      return response.data;
    } catch (error) {
      console.error('Error fetching market status:', error);
      throw error;
    }
  },

  async getIndicatorsByType(
    indicatorType: 'leading' | 'coincident' | 'lagging',
    startDate?: string,
    endDate?: string
  ): Promise<IndicatorsByTypeAPIResponse> {
    try {
      const timestamp = new Date().getTime(); 
      const response = await apiClient.get<IndicatorsByTypeAPIResponse>(`/v2/indicators/type/${indicatorType}`, {
        params: { start_date: startDate, end_date: endDate, _t: timestamp },
      });
      // The response.data.indicators are EnrichedIndicatorAPIResponse[], which now include ma_series_data
      return response.data;
    } catch (error) {
      console.error(`Error fetching ${indicatorType} indicators:`, error);
      throw error;
    }
  },

  async getMultipleIndicators(
    indicatorIds: string[],
    startDate?: string,
    endDate?: string
  ): Promise<Record<string, IndicatorData | null>> {
    const indicatorDataRecord: Record<string, IndicatorData | null> = {};
    const promises = indicatorIds.map(async (id) => {
      try {
        const data = await this.getIndicator(id, startDate, endDate);
        return { id, data };
      } catch (error) {
        console.error(`Error fetching indicator ${id} in getMultipleIndicators:`, error);
        return { id, data: null }; 
      }
    });

    const results = await Promise.all(promises);
    results.forEach(({ id, data }) => {
      indicatorDataRecord[id] = data;
    });
    return indicatorDataRecord;
  },
  
  async getISMPMI(startDate?: string, endDate?: string): Promise<IndicatorData> {
    return this.getIndicator('ISM-PMI', startDate, endDate);
  },
  async getISMNewOrders(startDate?: string, endDate?: string): Promise<IndicatorData> {
    return this.getIndicator('ISM-NEW-ORDERS', startDate, endDate);
  },
  async getVIXFromYahoo(startDate?: string, endDate?: string): Promise<IndicatorData> { 
    return this.getIndicator('VIX', startDate, endDate);
  },
  async getFREDData(seriesId: string, startDate?: string, endDate?: string): Promise<IndicatorData> {
    return this.getIndicator(seriesId, startDate, endDate);
  },
  async getGoldCopperRatio(startDate?: string, endDate?: string): Promise<IndicatorData> {
    return this.getIndicator('GOLD-COPPER-RATIO', startDate, endDate);
  },
  async getSP500Data(startDate?: string, endDate?: string): Promise<IndicatorData> { 
    return this.getIndicator('SP500', startDate, endDate);
  },

  async getIndicatorsByCategoryName(categoryName: string): Promise<string[]> {
    try {
      const categories = await this.getCategories(); 
      const foundCategory = categories.find(cat => cat.name === categoryName || cat.category_id === categoryName);
      return foundCategory ? foundCategory.indicators : [];
    } catch (error) {
      console.error(`Error fetching indicators for category name ${categoryName}:`, error);
      return [];
    }
  },

  async getCategoryIndicatorData(
    categoryName: string,
    startDate?: string,
    endDate?: string
  ): Promise<Record<string, IndicatorData | null>> {
    try {
      const indicatorIds = await this.getIndicatorsByCategoryName(categoryName);
      if (indicatorIds.length === 0) {
        console.warn(`No indicators found for category by name: ${categoryName}`);
        return {};
      }
      return this.getMultipleIndicators(indicatorIds, startDate, endDate);
    } catch (error) {
      console.error(`Error fetching data for category by name ${categoryName}:`, error);
      throw error; 
    }
  }
};

export default IndicatorService;


=== frontend\src\setupTests.ts ===

// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';


=== generate_codebase.bat ===

@echo off
setlocal enabledelayedexpansion

(
echo Directory Structure:
for /r %%d in (.) do (
    set "dir=%%d"
    rem --- Updated exclusion list ---
    echo !dir! | findstr /i /v "node_modules obj bin venv .venv env .env __pycache__ dist build .git .vscode" >nul
    if not errorlevel 1 (
        if exist "%%d" (
            pushd "%%d"
            dir /b /a-d 2>nul
            popd
        )
    )
)

echo.
echo File Contents:
for /r %%d in (.) do (
    set "dir=%%d"
    rem --- Updated exclusion list ---
    echo !dir! | findstr /i /v "node_modules obj bin venv .venv env .env __pycache__ dist build .git .vscode" >nul
    if not errorlevel 1 (
        if exist "%%d" (
            pushd "%%d"
            rem --- Updated file types list ---
            rem Removed *.cs, *.py.bak. Added *.yaml. Kept frontend files like *.tsx, *.ts, *.css, *.html.
            for %%i in (*.py *.md *.yml *.yaml *.json *.sh *.toml *.css *.tsx *.ts *.html *.bat *.ps1) do (
                if exist "%%i" (
                    echo.
                    echo === %%~fi ===
                    echo.
                    type "%%i" 2>nul
                )
            )
            popd
        )
    )
)

if exist Dockerfile (
    echo.
    echo === Dockerfile ===
    echo.
    type Dockerfile
)
) > full_codebase.txt

echo Full codebase snapshot created in full_codebase.txt
endlocal

=== generate_codebase.py ===

import os

# Configuration
output_filename = "full_codebase.txt"
excluded_dirs_simple = { # For top-level directory names to skip
    "node_modules", "obj", "bin", "__pycache__", ".git", ".vscode",
    "venv", ".venv", "env", ".env", "dist", "build",
    "coverage", ".pytest_cache", ".mypy_cache", ".ruff_cache",
    "site", "docs_build", # Add any other simple dir names
}
# For path segments anywhere in the path that should exclude the path
# This helps catch nested vendor/library folders if not caught by top-level name.
# Use with caution to avoid over-excluding.
excluded_path_segments = {
    os.path.join("lib", "python"), # Common pattern for venv site-packages
    os.path.join("Lib", "site-packages"), # Windows venv
    os.path.join("site-packages"),
    # Add more specific known library paths if needed, e.g., "node_modules" here too as a fallback
    "node_modules" 
}

included_extensions = {
    ".py", ".md", ".yml", ".yaml",  ".sh", ".toml",
    ".css", ".tsx", ".ts", ".html", ".bat", ".ps1",
    ".env.example" 
    # ".py.bak" # Typically excluded
}
root_dir = "."  # Current directory

def path_contains_excluded_segment(path_str: str, segments_to_exclude: set) -> bool:
    """Checks if any part of the path matches any of the excluded segments."""
    # Normalize path separators for consistent matching
    normalized_path = os.path.normpath(path_str)
    path_parts = set(normalized_path.split(os.sep))
    return not segments_to_exclude.isdisjoint(path_parts)


def create_codebase_snapshot():
    with open(output_filename, "w", encoding="utf-8", errors="ignore") as outfile:
        
        project_files_for_listing = []
        project_files_for_content = []

        for root, dirs, files in os.walk(root_dir, topdown=True):
            # 1. Prune recursion into top-level excluded directory names
            dirs[:] = [d for d in dirs if d not in excluded_dirs_simple]

            # 2. Check if the current 'root' itself is part of an excluded path segment
            # This helps skip processing files in already-entered excluded subdirectories deeper down
            # (e.g. if os.walk still yields a path like "some_dir/node_modules/some_file")
            relative_root_path = os.path.relpath(root, root_dir)
            if relative_root_path != "." and path_contains_excluded_segment(relative_root_path, excluded_path_segments):
                dirs[:] = [] # Don't recurse further into this path
                continue     # Don't process files in this root either

            # Prepare files from current directory for listing and content
            current_dir_files_listing = []
            for file_name in sorted(files):
                # Construct relative path for the file
                relative_file_path = os.path.join(relative_root_path, file_name) if relative_root_path != "." else file_name
                
                # Check again if this specific file's path is within an excluded segment
                if path_contains_excluded_segment(relative_file_path, excluded_path_segments):
                    continue

                _, file_extension = os.path.splitext(file_name.lower())
                
                # Add to directory listing
                current_dir_files_listing.append(file_name)
                
                # Decide if content should be added
                if file_extension in included_extensions:
                    absolute_file_path = os.path.join(root, file_name)
                    project_files_for_content.append((relative_file_path, absolute_file_path))
            
            if current_dir_files_listing:
                display_path = "Root Directory" if relative_root_path == "." else relative_root_path
                project_files_for_listing.append((display_path, current_dir_files_listing))

        # Write Directory Structure
        outfile.write("Directory Structure:\n")
        for display_path, files_in_dir in sorted(project_files_for_listing):
            outfile.write(f"{display_path}{os.sep}:\n")
            for file_name in files_in_dir:
                outfile.write(f"  {file_name}\n")
        
        # Write File Contents
        outfile.write("\nFile Contents:\n")
        project_files_for_content.sort() # Sort for consistent output

        for rel_path, abs_path in project_files_for_content:
            outfile.write(f"\n=== {rel_path} ===\n\n")
            try:
                with open(abs_path, "r", encoding="utf-8", errors="ignore") as infile:
                    outfile.write(infile.read())
                outfile.write("\n") 
            except Exception as e:
                outfile.write(f"Error reading file {rel_path}: {e}\n")

        # Handle Dockerfile specifically (if it exists in the root_dir)
        dockerfile_path_root = os.path.join(root_dir, "Dockerfile")
        if os.path.exists(dockerfile_path_root):
            is_already_added = any(rel_p == "Dockerfile" for rel_p, _ in project_files_for_content)
            if not is_already_added: # Only add if not already captured by extension rules
                outfile.write(f"\n=== Dockerfile ===\n\n")
                try:
                    with open(dockerfile_path_root, "r", encoding="utf-8", errors="ignore") as infile:
                        outfile.write(infile.read())
                    outfile.write("\n")
                except Exception as e:
                    outfile.write(f"Error reading Dockerfile: {e}\n")

    print(f"Codebase snapshot created: {output_filename}")

if __name__ == "__main__":
    create_codebase_snapshot()
